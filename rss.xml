
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:webfeeds="http://webfeeds.org/rss/1.0" xml:lang="en">
<title>Elie on Internet Security and Performance</title>
<subtitle>
Blog about web technologies and games with a focus performance and security.
</subtitle>
<webfeeds:cover image="https://www.elie.net/static/images/default.png"/>
<webfeeds:icon>https://www.elie.net/static/images/logo-elie.png</webfeeds:icon>
<webfeeds:logo>https://www.elie.net/static/images/logo-elie.svg</webfeeds:logo>
<webfeeds:accentColor>e9ebed</webfeeds:accentColor>
<webfeeds:analytics id="UA-112090-10" engine="GoogleAnalytics"/>
<webfeeds:related layout="card" target="browser"/>
<icon>https://www.elie.net/static/images/logo-elie.png</icon>
<updated>2018-05-30T13:00:00Z</updated>
<id>http://feeds.feedburner.com/ebursztein</id>
<link rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/ebursztein"/>
<link type="text/html" rel="alternate" href="https://www.elie.net"/>
<entry>
<published>2018-05-30T12:00:00Z</published>
<updated>2018-05-30T13:00:00Z</updated>
<id>
https://www.elie.net/blog/ai/attacks-against-machine-learning-an-overview?utm_source=rss
</id>
<title>Attacks against machine learning — an overview</title>
<content type="html">
<![CDATA[
<p> <img src="https://www.elie.net/static/images/banner/attacks-against-machine-learning-an-overview.png"/> </p> <p>This blog post survey the attacks techniques that target AI (artificial intelligence) systems and how to protect against them.</p> <p>At a high level, attacks against classifiers can be broken down into three types:</p> <ul> <li><strong> <a src="https://www.elie.net#toc-0">Adversarial inputs</a> </strong>, which are specially crafted inputs that have been developed with the aim of being reliably misclassified in order to evade detection. Adversarial inputs include malicious documents designed to evade antivirus, and emails attempting to evade spam filters.</li> <li><strong> <a src="https://www.elie.net#toc-10">Data poisoning attacks</a> </strong>, which involve feeding training adversarial data to the classifier. The most common attack type we observe is model skewing, where the attacker attempts to pollute training data in such a way that the boundary between what the classifier categorizes as good data, and what the classifier categorizes as bad, shifts in his favor. The second type of attack we observe in the wild is feedback weaponization, which attempts to abuse feedback mechanisms in an effort to manipulate the system toward misclassifying good content as abusive (e.g., competitor content or as part of revenge attacks).</li> <li><strong> <a src="https://www.elie.net#toc-17">Model stealing techniques</a> </strong>, which are used to “steal” (i.e., duplicate) models or recover training data membership via blackbox probing. This can be used, for example, to steal stock market prediction models and spam filtering models, in order to use them or be able to optimize more efficiently against such models.</li> </ul> <p>This post explores each of these classes of attack in turn, providing concrete examples and discussing potential mitigation techniques.</p> <p>This post is the fourth, and last, post in a series of four dedicated to providing a concise overview of how to use AI to build robust anti-abuse protections. The <a src="https://www.elie.net/blog/ai/harnessing-ai-to-combat-fraud-and-abuse-ai-is-the-key-to-robust-defenses">first post</a> explained why AI is key to building robust protection that meets user expectations and increasingly sophisticated attacks. Following the natural progression of building and launching an AI-based defense system, the <a src="https://www.elie.net/blog/ai/challenges-faced-while-training-an-ai-to-combat-abuse">second post</a> covered the challenges related to training classifiers. The <a src="https://www.elie.net/blog/ai/how-to-handle-mistakes-while-using-ai-to-block-attacks">third one</a> looked at the main difficulties faced when using a classifier in production to block attacks.</p> <p>This series of posts is modeled after the talk I gave at RSA 2018. Here is a re-recording of this talk:</p> <p> <div class="video"> <iframe width="560" height="315" src="https://www.youtube.com/embed/5gxI-6QmPdE" frameborder="0" allowfullscreen></iframe> </div> </p> <p>You can also get the <a href="https://elie.net/talk/how-to-successfully-harness-ai-to-combat-fraud-and-abuse">slides here</a> .</p> <p>Disclaimer: This post is intended as an overview for everyone interested in the subject of harnessing AI for anti-abuse defense, and it is a potential blueprint for those who are making the jump. Accordingly, this post focuses on providing a clear high-level summary, deliberately not delving into technical details. That said, if you are an expert, I am sure you’ll find ideas, techniques and references that you haven’t heard about before, and hopefully you’ll be inspired to explore them further.</p> <h2>Adversarial inputs</h2><p>Adversaries constantly probe classifiers with new inputs/payloads in an attempt to evade detection. Such payloads are called adversarial inputs because they are explicitly designed to bypass classifiers.</p> <p> <img src="https://www.elie.net/static/images/images/attacks-against-machine-learning-an-overview/gmail-adversarial-input.jpg" alt="Gmail Adversarial input"/> </p> <p>Here is a concrete example of an adversarial input: A few years back, one clever spammer realized that if the same multipart attachment appeared multiple times in an email, Gmail would only display the last attachment as visible in the screenshot above. He weaponized this knowledge by adding an invisible first multipart that contained many reputable domains in an attempt to evade detection. This attack is a variation of the class of attacks known as <a href="https://en.wikipedia.org/wiki/Keyword_stuffing">keyword stuffing</a> .</p> <p>More generally sooner or later classifiers face two types of adversarial input: mutated inputs, which are variations of a known attack specifically engineered to avoid your classifier, and zero-day inputs, which are never-seen-before payloads. Let’s explore each of these in turn.</p> <h3>Mutated inputs</h3><p> <img src="https://www.elie.net/static/images/images/attacks-against-machine-learning-an-overview/blackmarket-services.jpg" alt="Blackmarket services"/> </p> <p>Over the last few years we have seen an explosion of underground services designed to help cybercriminals craft undetectable payloads best known in the underworld as “FUD” (fully undetectable) ones. These services range from testing services that allow to test payloads against all anti-virus software, to automated packers that aim to obfuscate malicious documents in a way that makes them undetectable (with a warranty!). The screenshot above showcases two such services.</p> <p>This recrudescence of underground services specialized in payload crafting emphasizes the fact that:</p> <blockquote><p>Attackers actively optimize their attack to ensure they minimize classifiers detection rate.</p> </blockquote><p>Therefore, it is essential to develop detection systems in such a way that they makes it hard for attackers to perform payload optimization. Below are three key design strategies to help with that.</p> <h4>1.Limit information leakage</h4><p> <img src="https://www.elie.net/static/images/images/attacks-against-machine-learning-an-overview/strip-presque-le-mot-de-passe-english650-final.jpg" alt="CommitStrip"/> </p> <p>The goal here is to ensure that attackers gain as little insight as possible when they are probing your system. It’s important to keep the feedback minimal and delay it as much as possible, for example avoid returning detailed error codes or confidence values.</p> <h4>2. Limit probing</h4><p>The goal of this strategy is to slow down attackers by limiting how often many payload they can test against your systems. By restricting how much testing an attacker can perform against your systems, you’ll effectively reduce the rate at which they can devise harmful payloads.</p> <p> <img src="https://www.elie.net/static/images/images/attacks-against-machine-learning-an-overview/stackoverflow-captcha.png" alt="StackOverflow captcha"/> </p> <p>This strategy is mostly carried out by implementing rate limiting on scarce resources such as IP and accounts. A classical example of such rate limiting is to ask the user to solve a CAPTCHA if he is posting too frequently as illustrated above.</p> <p> <img src="https://www.elie.net/static/images/images/attacks-against-machine-learning-an-overview/account-blackmarket.png" alt="Account Blackmarket"/> </p> <p>The negative side effect of such active rate limiting is that it creates an incentive for bad actors to create fake accounts and use compromised user computers to diversify their pool of IPs. The widespread use of rate limiting through the industry is a major driving factor behind the rise of very <a href="https://elie.net/publication/framing-dependencies-introduced-by-underground-commoditization">active blackmarket</a> forums where accounts and IP addresses are routinely sold, as visible in the screenshot above.</p> <h4>3. Ensemble learning</h4><p>Last, but not least, it’s important to combine various detection mechanisms to make it harder for attackers to bypass the overall system. Using <a href="https://en.wikipedia.org/wiki/Ensemble_learning">ensemble learning</a> to combine different type of detection methods, such as reputation-based ones, AI classifiers, detection rules and anomaly detection, improves the robustness of your system because bad actors have to craft payloads that avoid all those mechanisms at once.</p> <p> <img src="https://www.elie.net/static/images/images/attacks-against-machine-learning-an-overview/gmail-classifier-breakdown.jpg" alt="Gmail classifier breakdown"/> </p> <p>For example as shown in the screenshot above, to ensure Gmail classifier robustness against spammers we combine multiple classifiers and auxiliary systems. Such systems include a reputation system, a large linear classifier, a deep learning classifier and a few other secret techniques ;)</p> <h4>Examples of adversarial attacks against deep neural networks</h4><p> <img src="https://www.elie.net/static/images/images/attacks-against-machine-learning-an-overview/adversarial_input-example.jpg" alt="Adversarial input example"/> </p> <p>A very active related research field is how to craft <a href="https://arxiv.org/abs/1312.6199">adversarial examples</a> that fool deep-neural networks (DNNs). It is now trivia to create imperceptible perturbation that completely fools DNN as shown in the screenshot above, taken from <a href="https://arxiv.org/abs/1412.6572">this paper</a> .</p> <p> <a href="https://arxiv.org/abs/1711.11561">Recent work</a> suggests that CNN are vulnerable to adversarial input attack because they tend to learn superficial dataset regularity instead of generalizing well and learning high-level representation that would be less susceptible to noise.</p> <p> <div class="video"> <iframe width="560" height="315" src="https://www.youtube.com/embed/r2jm0nRJZdI" frameborder="0" allowfullscreen></iframe> </div> </p> <p>This type of attack affects all DNN, <a href="https://arxiv.org/abs/1701.04143">including reinforcement-based ones</a> , as highlighted in the video above. To learn more about such attacks, you should read Ian’s intro <a href="https://blog.openai.com/adversarial-example-research/">post on the subject</a> or start experimenting with <a href="https://github.com/tensorflow/cleverhans">Clever Hans</a> .</p> <p>From a defender perspective, this type of attack has proven (so far) to be very problematic because we don’t yet have an effective way of defending against such attacks. Fundamentally, we don’t have an efficient way to get DNNs to generate good output for <em>all inputs</em>. Getting them to do so is incredibly hard because DNNs perform nonlinear/non-convex optimizations within very large spaces and we have yet to teach them to learn high level representation that generalize well. You can read <a href="http://www.cleverhans.io/security/privacy/ml/2017/02/15/why-attacking-machine-learning-is-easier-than-defending-it.html">Ian and Nicolas’ in-depth post</a> to know more about this.</p> <h3>Zero-day inputs</h3><p>The other obvious type of adversarial inputs that can completely throw off classifiers are new attacks. New attacks don’t happen very often, but its’ still important to know how to deal with them as they can be quite devastating.</p> <p> <img src="https://www.elie.net/static/images/images/attacks-against-machine-learning-an-overview/incentive-for-new-attacks.jpg" alt="Incentive for new attacks"/> </p> <p>While there are many unpredictable underlying reasons why new attacks emerge, in our experience the following two types of events are likely to trigger their emergence:</p> <p><strong>New product or feature launch</strong>: By nature adding functionalities opens up new attack surfaces that attackers are very quick to probe. This is why it is essential (and yet hard) to provide day zero defense when a new product launches.</p> <p><strong>Increased incentive</strong>: While rarely discussed, many new attacks surge are driven by an attack vector becoming very profitable. A very recent example of such behavior is the rise of abusing cloud services such as Google Cloud to mine cryptocurrencies in response to the surge of bitcoin price late 2017.</p> <p>As <a href="https://www.coindesk.com/price/">bitcoin prices</a> skyrocketed past $10,000, we saw a surge of new attacks that attempted to steal Google cloud compute resources to mine. I will cover how we detected those new attacks a little later in this post.</p> <p> <img src="https://www.elie.net/static/images/images/attacks-against-machine-learning-an-overview/black-swan-event.png" alt="Black Swan"/> </p> <p>All in all, the <a href="https://en.wikipedia.org/wiki/Black_swan_theory">Black Swan theory</a> <a href="https://amzn.to/2schXAb">formalized by Nassim Taleb</a> applies to AI-based defenses, as it does with any type of defense:</p> <blockquote><p>Sooner or later an unpredictable attack will throw off your classifier and it will have a major impact.</p> </blockquote><p>However, it is not because you can’t predict which attacks will throw off your classifier, or when such an attack will strike that you are powerless. You can plan around such attacks happening and put in place contingency plans to mitigate it. Here are a few directions to explore while preparing for black swan events.</p> <h4>1. Develop an incident response process</h4><p>The first thing to do is to develop and test an incident recovery process to ensure you react appropriately when you get caught off guard. This includes, but is not limited to, having the necessary controls in place to delay or halt processing while you are debugging your classifiers, and knowing who to call.</p> <p>The (free) Google SRE (Site Reliability Engineering) handbook has a chapter on <a href="https://landing.google.com/sre/book/chapters/managing-incidents.html">managing incidents</a> , and another on <a href="https://landing.google.com/sre/book/chapters/emergency-response.html">emergency responses</a> . For a more cybersecurity-centric document you should look at the <a href="https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-184.pdf">NIST (National Institute of Standards and Technology) cybersecurity event recovery guide</a> . Finally, if you’d rather watch a talk instead, take a look at the video on <a href="https://www.usenix.org/conference/lisa15/conference-program/presentation/krishnan">how Google runs its Disaster Recovery Training (DiRT) program</a> , and the video on how <a href="https://www.usenix.org/node/197445">Facebook do incident response</a> (the recording don’t show the slides)</p> <h4>2. Use transfer learning to protect new products</h4><p>The obvious key difficult is that you don’t have past data to train your classifiers on. One way to mitigate this issue is to make use of <a href="http://ruder.io/transfer-learning/">transfer learning</a> , which allows you to reuse already existing data from one domain, and apply it to another.</p> <p>For example, if you’re dealing with images you can leverage an existing <a href="https://keras.io/applications/">pre-trained model</a> , while if you’re dealing with text you can use public datasets such as the <a href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge">Jigsaw dataset of toxic comments</a> .</p> <h4>3. Leverage anomaly detection</h4><p>Anomaly detection algorithms can be used as a first line of defense because, by nature, a new attack will create a never previously encountered set of anomalies related to how they exploit your system.</p> <p> <img src="https://www.elie.net/static/images/images/attacks-against-machine-learning-an-overview/windfall-lottery.jpg" alt="Windfall lottery"/> </p> <p>An historical example of a novel attack triggering a swath of new anomalies was the “MIT gambling syndicate” attack against the <a href="https://www.theatlantic.com/business/archive/2016/02/how-mit-students-gamed-the-lottery/470349/">Massachusetts WinFall lottery game</a> .</p> <p>Back in 2005, multiple groups of gambling syndicates discovered a flaw in the WinFall lottery system: when the jackpot was split among all participants, you would earn $2.3 on average for each $2 ticket you bought. This split, known as a “roll-down,” occurred every time the pool of money exceeded $2 million.</p> <p>To avoid sharing the gains with other groups, the MIT gang decided to trigger a roll-down way ahead of time by performing a massive buyout of tickets three weeks before a roll-down was expected. Obviously, this <a href="https://www.bostonglobe.com/metro/2012/07/30/inspector-general-says-lottery-allowed-gambling-syndicates-take-over-winfall-game/Zc19kLVGEj92LkK75SHD7J/story.html">massive surge of tickets</a> —bought from very few retailers—created a host of anomalies that were detected by the lottery organization.</p> <p> <img src="https://www.elie.net/static/images/images/attacks-against-machine-learning-an-overview/bitcoin-visual.jpg" alt="Bitcoin visual"/> </p> <p>More recently, as alluded to earlier in this post, when Bitcoin prices rose like crazy in 2017 we started to see an army of bad actors trying to benefit from this surge by mining using Google cloud instances for free. To acquire instances for “free,” they attempted to exploit many attack vectors that included trying to abuse our free tier, use stolen credit cards, compromise legitimate cloud users’ computers, and hijack cloud users’ accounts via phishing.</p> <p> <img src="https://www.elie.net/static/images/images/attacks-against-machine-learning-an-overview/youtube-tutorial-cloud-mining.jpg" alt="Youtube tutorial for cloud mining"/> </p> <p>Very quickly, this type of attack became so popular that it led to thousands of people watching YouTube tutorials on how to mine on Google cloud (which is unprofitable under normal circumstances). Obviously, we couldn’t anticipate that abusive mining would become such a huge issue.</p> <p> <img src="https://www.elie.net/static/images/images/attacks-against-machine-learning-an-overview/cloud-miner-detection.jpg" alt="Cloud miner detection"/> </p> <p>Fortunately, we did have an <a href="https://cloudplatform.googleblog.com/2018/03/monitor-your-GCP-environment-with-Cloud-Security-Command-Center.html">anomaly detection system in place</a> for Google Cloud instances when that happened. As expected, and shown in the chart above which was taken directly from our anomaly detection system dashboard, it turns out that when instances start mining their temporal behavior shift drastically because the associated resource usage is fundamentally different from the traditional resource usage exhibited by uncompromised cloud instances. We were able to use this shift detection to curb this new vector of attacks, ensure that our cloud platform remains stable and warm GCE clients that they were compromised.</p> <h2>Data poisoning</h2><p>The second class of attacks faced by classifiers relates to adversaries attempting to poison your data to make your system misbehave.</p> <h3>Model skewing</h3><p>The first type of poisoning attack is called model skewing, where attackers attempt to pollute training data to shift the learned boundary between what the classifier categorizes as good input, and what the classifier categorizes as bad input. For example, model skewing can be used to try to pollute training data to trick the classifier to mark specific malicious binaries as benign.</p> <h4>Concrete example</h4><p> <img src="https://www.elie.net/static/images/images/attacks-against-machine-learning-an-overview/gmail-adversarial-skewing-attack.png" alt="Gmail adversarial skewing attack"/> </p> <p>In practice, we regularly see some of the most advanced spammer groups trying to throw the Gmail filter off-track by reporting massive amounts of spam emails as not spam. As shown in the figure, between the end of Nov 2017 and early 2018, there were at least four malicious large-scale attempts to skew our classifier.</p> <p>Thus, when designing an AI base defense, you need to account for the fact that:</p> <blockquote><p>Attackers actively attempt to shift the learned boundary between abusive and legitimate use in their favor.</p> </blockquote><h4>Mitigation strategies</h4><p>To prevent attackers from skewing models, you can leverage the following three strategies:</p> <ul> <li><strong>Use sensible data sampling</strong>: You need to ensure that a small group of entities, including IPs or users, can’t account for a large fraction of the model training data. In particular, be cautious to not over-weighting false positives and false negatives reported by users. This can potentially be achieved by limiting the number of examples that each user can contribute, or using decaying weights based on the number of examples reported.</li> <li><strong>Compare your newly trained classifier to the previous one</strong> to estimate how much has changed. For example, you can perform a <a href="https://tech.co/the-dark-launch-how-googlefacebook-release-new-features-2016-04">dark launch</a> and compare the two outputs on the same traffic. Alternative options include A/B testing on a fraction of the traffic, and <a href="https://en.wikipedia.org/wiki/Backtesting">backtesting</a> .</li> <li><strong>Build a golden dataset</strong> that your classifier must accurately predict in order to be launched into production. This dataset ideally contains a set of curated attacks and normal content that are representative of your system. This process will ensure that you can detect when a weaponization attack was able to generate a significant regression in your model before it negatively impacted your users.</li> </ul> <h3>Feedback weaponization</h3><p>The second type of data poisoning attack is the weaponization of user feedback systems to attack legitimate users and content. As soon as attackers realize that you are using user feedback, one way or another—for penalization purposes—they will try to exploit this fact to their advantage.</p> <h4>Concrete examples</h4><p> <img src="https://www.elie.net/static/images/images/attacks-against-machine-learning-an-overview/operation-mobile-assault.jpg" alt="Mobile assault"/> </p> <p>One of the most egregious attempts to weaponize user feedback we witnessed in 2017 was a group of <a href="https://www.huffingtonpost.com/entry/how-trump-supporters-tanked-cnns-app-with-1-star-ratings_us_597c9f82e4b06b305561d11c">4chan users that decided to tank the CNN app ranking</a> on the Play Store and App Store by leaving thousands of 1-star ratings.</p> <p> <img src="https://www.elie.net/static/images/images/attacks-against-machine-learning-an-overview/blackmarket-feedback-weaponization.jpg" alt="Blackmarket feedback weaponization"/> </p> <p>Feedback weaponization is actively used by bad actors for a number of reasons, including: attempting to take down the competition, exacting revenge, and covering their tracks. The screenshot above showcases a blackmarket post discussing how to “use Google” to take out a competitor.</p> <p>Accordingly, while building your system you need to work under the assumption that:</p> <blockquote><p>Any feedback mechanism will be weaponized to attack legitimate users and content.</p> </blockquote><h4>Mitigation strategies</h4><p>Here are the two key points to keep in mind while working on mitigating feedback weaponization:</p> <ul> <li><strong>Don’t create a direct loop between feedback and penalization.</strong> Instead, make sure the feedback authenticity is assessed and combined with other signals before making a decision.</li> <li><strong>Don’t assume that the owner of the content that is benefiting from the abuse is responsible for it.</strong> For example, it’s not because a photo has hundreds of fake likes that the owner may have bought it. We have seen countless cases where attackers juiced up legitimate content in an attempt to cover their tracks or try to get us to penalize innocent users.</li> </ul> <h2>Model-stealing attacks</h2><p>This post would not be complete without mentioning attacks that aim to recover models or information about the data used during training. Such attacks are a key concern because models represent valuable intellectual property assets that are trained on some of a company’s most valuable data, such as financial trades, medical information or user transactions.</p> <p>Ensuring the security of models trained on user-sensitive data such as cancer-related data is paramount as such models can potentially be abused to <a href="https://www.cs.cornell.edu/~shmat/shmat_oak17.pdf">disclose sensitive user information</a> .</p> <h3>Attacks</h3><p>The two main model-stealing attacks are:</p> <p><strong>Model reconstruction</strong>: The key idea here is that the attacker is able to recreate a model by probing the public API and gradually refining his own model by using it as an Oracle. A <a href="https://www.usenix.org/system/files/conference/usenixsecurity16/sec16_paper_tramer.pdf">recent paper</a> showed that such attacks appear to be effective against most AI algorithms, including SVM, Random Forests and deep neural networks.</p> <p><strong>Membership leakage</strong>: Here, the attacker builds shadow models that enable him to determine whether a given record was used to train a model. While such attacks don’t recover the model, they potentially disclose sensitive information.</p> <h3>Defense</h3><p> <img src="https://www.elie.net/static/images/images/attacks-against-machine-learning-an-overview/pate-aggregation.png" alt="PATE aggregation"/> </p> <p>The best-known defense against model-stealing attacks is known as <a href="https://arxiv.org/abs/1610.05755">PATE</a> ( <a href="https://arxiv.org/abs/1802.08908">latest paper</a> ) —a privacy framework developed by Ian Goodfellow et al. As shown in the figure above, the key idea behind PATE is to partition the data and train multiple models that are combined to make a decision. This decision is then fudged with noise like other <a href="https://en.wikipedia.org/wiki/Differential_privacy">differential privacy systems</a> .</p> <p>To know more about differential privacy read <a href="https://blog.cryptographyengineering.com/2016/06/15/what-is-differential-privacy/">Matt's intro post</a> . To know more about PATE and model stealing attacks read <a href="http://www.cleverhans.io/privacy/2018/04/29/privacy-and-machine-learning.html">Ian's post</a> on the subject.</p> <h2>Conclusion</h2><p>It’s about time to wrap up this (rather long!) series of posts on how to use AI to combat fraud and abuse. The key takeaway of this series (as detailed in the first post) is that:</p> <blockquote><p>AI is key to building protections that keep up with both users’ expectations and increasingly sophisticated attacks.</p> </blockquote><p>As discussed in this post and the two previous ones, there are some challenges to overcome to make this work in practice. However, now that <a href="https://keras.io/">AI frameworks</a> are mature and well documented, there’s never been a better time to start using AI in your defense systems, so don’t let those challenges stop you as the upsides are very strong.</p> <p>Thank you for reading this blog post up to the end. And please don’t forget to share it, so your friends and colleagues can also learn how to use AI for anti-abuse purposes.</p> <p>To get notified when my next post is online, follow me on <a href="https://twitter.com/elie">Twitter</a> , <a href="https://www.facebook.com/elieblog">Facebook</a> , <a href="https://plus.google.com/+ElieBursztein">Google+</a> , or <a href="https://www.linkedin.com/in/bursztein/">LinkedIn</a> . You can also get the full posts directly in your inbox by subscribing to the mailing list or via <a href="http://feeds.feedburner.com/ebursztein">RSS</a> .</p> <p>A bientôt!</p>
]]>
</content>
<link type="text/html" rel="alternate" href="https://www.elie.net/blog/ai/attacks-against-machine-learning-an-overview?utm_source=rss"/>
<author>
<name>Elie Bursztein</name>
<email>feed@elie.net</email>
</author>
</entry>
<entry>
<published>2018-04-29T09:00:00Z</published>
<updated>2018-05-29T09:00:00Z</updated>
<id>
https://www.elie.net/blog/ai/how-to-handle-mistakes-while-using-ai-to-block-attacks?utm_source=rss
</id>
<title>
How to handle mistakes while using AI to block attacks
</title>
<content type="html">
<![CDATA[
<p> <img src="https://www.elie.net/static/images/banner/how-to-handle-mistakes-while-using-ai-to-block-attacks.png"/> </p> <p>This post looks at the main difficulties faced while using a classifier to block attacks: handling mistakes and uncertainty such that the overall system remains secure and usable.</p> <p>At a high level, the main difficulty faced when using a classifier to block attacks is how to handle mistakes. The need to handle errors correctly can be broken down into two challenges: how to strike the right balance between false positives and false negatives, to ensure that your product remains safe when your classifier makes an error; and how to explain why something was blocked, both to inform users and for debugging purposes. This post explore those two challenges in turn.</p> <p>This post is the third post in a series of four that is dedicated to providing a concise overview of how to use artificial intelligence (AI) to build robust anti-abuse protections. <a src="https://www.elie.net/blog/ai/harnessing-ai-to-combat-fraud-and-abuse-ai-is-the-key-to-robust-defenses">The first post</a> explains why AI is key to build robust anti-defenses that keep up with user expectations and increasingly sophisticated attackers. Following the natural progression of building and launching an AI-based defense system, <a src="https://www.elie.net/blog/ai/challenges-faced-while-training-an-ai-to-combat-abuse">the second post</a> covers the challenges related to training and the <a src="https://www.elie.net/blog/ai/attacks-against-machine-learning-an-overview">fourth and final post</a> looks at how attackers go about attacking AI-based defenses.</p> <p>This series of posts is modeled after the talk I gave at <a src="https://www.elie.net/talk/how-to-successfully-harness-ai-to-combat-fraud-and-abuse/">RSA 2018</a> . Here is a re-recording of this talk:</p> <p> <div class="video"> <iframe width="560" height="315" src="https://www.youtube.com/embed/5gxI-6QmPdE" frameborder="0" allowfullscreen></iframe> </div> </p> <p>You can also get the <a href="https://elie.net/talk/how-to-successfully-harness-ai-to-combat-fraud-and-abuse">slides here.</a> </p> <p><strong>Disclaimer</strong>: This series is intended as an overview for everyone interested in the subject of harnessing AI for anti-abuse defense, and it is a potential blueprint for those who are making the jump. Accordingly, this series focuses on providing a clear high-level summary, deliberately not delving into technical details. That being said, if you are an expert, I am sure you will find ideas and techniques that you haven’t heard about before, and hopefully you will be inspired to explore them further.</p> <p>Let’s get started!</p> <h2>Striking the right balance between false positives and false negatives</h2><p>The single most important decision you have to make when putting a classifier in production is how to balance your classifier error rates. This decision deeply affects the security and usability of your system. This struggle is best understood through a real-life example, so let’s start by looking at one of the toughest: <a href="https://elie.net/publication/secrets-lies-and-account-recovery-lessons-from-the-use-of-personal-knowledge-questions-at-google">the account recovery process.</a> </p> <p> <img src="https://www.elie.net/static/images/images/how-to-handle-mistakes-while-using-ai-to-block-attacks/account-recovery-fp-vs-fn.png" alt="Account Recovery"/> </p> <p>When a user loses access to their account, they have the option to go through the account recovery process, supply information to prove they are who they claim to be, and get their account access back. At the end of the recovery process the classifier has to decide, based on the information provided and other signals, whether or not to let the person claiming to be the user recover the account.</p> <p>The key question here is what the classifier should do when it is not clear what the decision should be. Technically, this is done by adjusting the <a href="https://en.wikipedia.org/wiki/Confusion_matrix">false positive and false negative rates</a> ; this is also known as classifier <a href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity">sensitivity and specificity</a> . There are two options:</p> <ol> <li><strong>Make the classifier cautious</strong>, which is to favor reducing false positives (hacker break-in) at the expense of increasing false negatives (legitimate user denied).</li> <li><strong>Make the classifier optimistic</strong>, which is to favor reducing false negatives (legitimate user denied) at the expense of increasing false positives (hacker break-in)</li> </ol> <p>While both types of error are bad, it is clear that for account recovery, letting a hacker break into a user’s account is not an option. Accordingly, for that specific use case, the classifier must be tuned to err on the cautious side. Technically, this means we are willing to reduce the false positive rate at the expense of slightly increasing the false negative rate.</p> <p> <img src="https://www.elie.net/static/images/images/how-to-handle-mistakes-while-using-ai-to-block-attacks/false-negative-false-positive-rate.png" alt="False negative vs False positive"/> </p> <p>It is important to note that the relation between a false positive and a false negative is not linear, as illustrated in the figure above. In practical terms, this means that the more you reduce one at the expense of the other, the higher your overall error rate will be. There is <a href="https://en.wikipedia.org/wiki/No_free_lunch_theorem">no free lunch</a> :-)</p> <p>For example, you might be able to reduce the false negative rate from 0.3% to 0.2% by slightly increasing the false positive rate from 0.3% to 0.42%. However, reducing the false negative rate further, to 0.1%, will increase your false positive rate to a whopping 2%. Those numbers are made up but they do illustrate how the nonlinear relation that exists between false positives and false negatives plays out.</p> <p>To sum up, the first challenge faced when using classifiers in production to detect attack is that:</p> <blockquote><p>In fraud and abuse the margin for error is often nonexistent, and some error types are more costly than others.</p> </blockquote><p>This challenge is addressed by paying extra attention to how classifier error rates are balanced, to ensure that your systems are as safe and usable as possible. Here are three key points you need to consider when balancing your classifier:</p> <ol> <li><strong>Use manual reviews</strong>:When the stakes are high and the classifier is not confident enough, it might be worth relying on a human to make the final determination.</li> <li><strong>Adjust your false positive and false negative rates</strong>: Skew your model errors in one direction or the other, adjusting it so that it errs on the correct side to keep your product safe.</li> <li><strong>Implement catch-up mechanisms</strong>: No classifier is perfect, so implementing catch-up mechanisms to mitigate the impact of errors is important. Catch-up mechanisms include an appeal system and in-product warnings.</li> </ol> <p>To wrap up this section, let’s consider how Gmail spam classifier errors are balanced.</p> <p> <img src="https://www.elie.net/static/images/images/how-to-handle-mistakes-while-using-ai-to-block-attacks/gmail-fn-vs-fp.png" alt="False negative vs False positive in Gmail"/> </p> <p>Gmail users really don’t want to miss an important email but are okay with spending a second or two to get rid of spam in their inboxes, provided it doesn’t happen too often. Based on this insight, we made the conscious decision to bias the Gmail spam classifier to ensure that the false positives rate (which means good emails that end up in the spam folder) is as low as possible. Reducing the <a href="https://gmail.googleblog.com/2015/07/the-mail-you-want-not-spam-you-dont.html">false positives rate to 0.05%</a> is achieved at the expense of a slightly higher false negatives rate (which means spam in user inboxes) of 0.1%.</p> <h2>Predicting is not explaining</h2><p>Our second challenge is that being able to predict if something is an attack does not mean you are able to explain why it was detected.</p> <blockquote><p>Classification is a binary decision. Explaining it requires additional information.</p> </blockquote><p>Fundamentally, dealing with attacks and abuse attempts is a binary decision: you either block something or you don’t. However, in many cases, especially when the classifier makes an error, your users will want to know why something was blocked. Being able to explain how the classifier reached a particular decision requires additional information that must be gathered through additional means.</p> <p>Here are three potential directions that will allow you to collect the additional information you need to explain your classifier’s decisions.</p> <h3>1. Use similarity to known attacks</h3><p> <img src="https://www.elie.net/static/images/images/how-to-handle-mistakes-while-using-ai-to-block-attacks/linear-relationships.png" alt="Linear relationships"/> </p> <p>First you can look at how similar a given blocked attack is to known attacks. If it is very similar to one of them, then it is very likely that the blocked attack was a variation of it. Performing this type of explanation is particularly easy when your <a href="https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture">model uses embedding</a> because you can directly apply distance computation to those embedding to find related items. This has been applied successfully to <a href="https://www.tensorflow.org/tutorials/word2vec">words</a> and <a href="https://arxiv.org/pdf/1503.03832.pdf">faces</a> for example.</p> <h3>2. Train specialized models</h3><p>Instead of having a single model that classifies all attacks, you can use a collection of more specialized models that target specific classes of attacks. Splitting the detection into multiple classifiers makes it easier to attribute a decision to a specific attack class because there is a one-to-one mapping between the attack type and the classifier that detected it. Also, in general, specialized models tend to be more accurate and easier to train, so you should rely on those if you can.</p> <h3>3. Leverage model explainability</h3><p> <img src="https://www.elie.net/static/images/images/how-to-handle-mistakes-while-using-ai-to-block-attacks/attention-map-example.jpg" alt="Attention map example"/> </p> <p>Last but not least, you can <a href="https://medium.com/@BonsaiAI/explainable-ai-3-deep-explanations-approaches-to-xai-1807e251e537">analyze the inner state</a> of the model to glean insights about why a decision was made. As you can see in the screenshot above, for example, a <a href="http://cnnlocalization.csail.mit.edu/">class-specific saliency map</a> (see this recent <a href="https://arxiv.org/pdf/1610.02391v1.pdf">paper</a> ) helps us to understand which section of the image contributed the most to the decision. Model explainability is a very active field of research and a lot of great <a href="https://github.com/raghakot/keras-vis">tools</a> , <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Lu_Knowing_When_to_CVPR_2017_paper.pdf">techniques</a> and <a href="https://arxiv.org/pdf/1704.07911.pdf">analysis</a> have been released recently.</p> <p> <img src="https://www.elie.net/static/images/images/how-to-handle-mistakes-while-using-ai-to-block-attacks/cmap-with-captioning.png" alt="Cmap"/> </p> <h3>Gmail as an example</h3><p> <img src="https://www.elie.net/static/images/images/how-to-handle-mistakes-while-using-ai-to-block-attacks/gmail-red-banner-example.png" alt="Gmail Banner warning"/> </p> <p>Gmail makes use of explainability to help users understand better why something is in the spam folder and why it is dangerous. As visible in the screenshot above, on top of each a spam email we add a red banner that explains why the email is dangerous, and does so in simple terms meant to be understandable to every user.</p> <h2>Conclusion</h2><p>Overall this post can be summarized as follow:</p> <blockquote><p>Successful applying AI to abuse fighting requires to handle classifier errors is a safe way and be able to understand how a particular decision was reached.</p> </blockquote><p>The <a src="https://www.elie.net/blog/ai/attacks-against-machine-learning-an-overview">next post</a> of the serie discusses the various attacks against classifiers and how to mitigate them.</p> <p>Thank you for reading this post till the end! If you enjoyed it, don’t forget to share it on your favorite social network so that your friends and colleagues can enjoy it too and learn about AI and anti-abuse.</p> <p>To get notified when my next post is online, follow me on <a href="https://twitter.com/elie">Twitter</a> , <a href="https://www.facebook.com/elieblog">Facebook</a> , <a href="https://plus.google.com/+ElieBursztein">Google+</a> , or <a href="https://www.linkedin.com/in/bursztein/">LinkedIn</a> . You can also get the full posts directly in your inbox by subscribing to the mailing list or via <a href="http://feeds.feedburner.com/ebursztein">RSS</a> .</p> <p>A bientôt!</p>
]]>
</content>
<link type="text/html" rel="alternate" href="https://www.elie.net/blog/ai/how-to-handle-mistakes-while-using-ai-to-block-attacks?utm_source=rss"/>
<author>
<name>Elie Bursztein</name>
<email>feed@elie.net</email>
</author>
</entry>
<entry>
<published>2018-04-23T09:00:00Z</published>
<updated>2018-05-29T09:00:00Z</updated>
<id>
https://www.elie.net/blog/ai/challenges-faced-while-training-an-ai-to-combat-abuse?utm_source=rss
</id>
<title>
Challenges faced while training an AI to combat abuse
</title>
<content type="html">
<![CDATA[
<p> <img src="https://www.elie.net/static/images/banner/challenges-faced-while-training-an-ai-to-combat-abuse.png"/> </p> <p>This post looks at the main challenges that arise when training a classifier to combat fraud and abuse.</p> <p>At a high level, what makes training a classifier to detect fraud and abuse unique is that it deals with data generated by an adversary that actively attempts to evade detection. Sucessfully training a classifier is such adversarial settings requires to overcome the following four challenges:</p> <ol> <li> <a src="https://www.elie.net#toc-0">Non stationarity</a> Abuse-fighting is a <a href="https://www.oxfordmartin.ox.ac.uk/downloads/briefings/All_Change.pdf">non-stationary</a> problem because attacks never stop evolving. This evolution renders past trainnig data obsolete and force classifiers to be retrained continiously to remain accurate.</li> <li> <a src="https://www.elie.net#toc-5">Lack of ground truth</a> : Collecting accurate attack training data is very difficult because by nature attackers attempts to conceal their activities.</li> <li> <a src="https://www.elie.net#toc-9">Ambigious data taxonomy</a> : Anti-abuse classifiers have to be very accurate despite dealing with data and taxonomy ambiguity. When you think about it even the well established concept of spam is ill defined.</li> <li> <a src="https://www.elie.net#toc-10">Lack of obvious features</a> Last, but not least as defenders we have to defend all products which lead us to find way to apply AI to products that are not AI friendly because they lack rich content and features. </li> </ol> <p>This post explore each of these challenges in turn.</p> <p>This post is the <em>second post</em> of a series of four that is dedicated to provide a concise overview of how to harness AI to build robust anti-abuse protections. <a src="https://www.elie.net/blog/ai/harnessing-ai-to-combat-fraud-and-abuse-ai-is-the-key-to-robust-defenses">The first post</a> explains why AI is key to build robust anti-defenses that keep up with user expectations and increasingly sophisticated attackers. Following the natural progression of building and launching an AI-based defense system, the <a src="https://www.elie.net/blog/ai/how-to-handle-mistakes-while-using-ai-to-block-attacks">third post</a> examines classification issues, and the <a src="https://www.elie.net/blog/ai/attacks-against-machine-learning-an-overview">last post</a> looks at how attackers go about attacking AI-based defenses.</p> <p>This series of posts is modeled after <a src="https://www.elie.net/talk/how-to-successfully-harness-ai-to-combat-fraud-and-abuse">the talk I gave at RSA 2018</a> . Here is a re-recording of this talk:</p> <p> <div class="video"> <iframe width="560" height="315" src="https://www.youtube.com/embed/5gxI-6QmPdE" frameborder="0" allowfullscreen></iframe> </div> </p> <p>You can also get the <a src="https://www.elie.net/talk/how-to-successfully-harness-ai-to-combat-fraud-and-abuse/">slides here</a> .</p> <p><strong>Disclaimer</strong>: This series is meant to provide an overview for everyone interested in the subject of harnessing AI for anti-abuse defense, and it is a potential blueprint for those who are making the jump. Accordingly, this series focuses on providing a clear high-level summary, purposely avoiding delving into technical details. That being said, if you are an expert, I am sure you will find ideas and techniques that you haven’t heard about before, and hopefully you will be inspired to explore them further.</p> <p>Let’s get started!</p> <h2>Non-stationary problem</h2><p>Traditionally, when applying AI to a given problem, you are able to reuse the same data over and over again because the problem definition is stable. This is not the case when combating abuse because attacks never stop evolving. As a result to ensure that anti-abuse classifiers remain accurate, their training data need to be constantly refreshed to incorporate the latest type of attacks.</p> <p>Let me give you a concrete example so it is clear what the difference between a stable problem and an <a href="https://www.oxfordmartin.ox.ac.uk/downloads/briefings/All_Change.pdf">unstable/non-stationary one</a> is.</p> <p> <img src="https://www.elie.net/static/images/images/challenges-faced-while-training-an-ai-to-combat-abuse/cat-classifier.jpg" alt="cat example"/> </p> <p>Let’s say you would like to create a classifier that recognizes cats and other animals. This is considered to be a stable problem because animals are expected to look roughly the same for the next few hundred years (barring a nuclear war). Accordingly, to train this type of classifier, you only need to collect and annotate animal images once at the beginning of the project.</p> <p> <img src="https://www.elie.net/static/images/images/challenges-faced-while-training-an-ai-to-combat-abuse/phishing-page-through-the-ages.jpg" alt="Phishing through ages"/> </p> <p>On the other hand, if you would like to train a classifier that recognizes phishing pages, this “collect once” approach doesn’t work because phishing pages keep evolving and look drastically different over time, as visible in the screenshot above.</p> <p>More generally, while training classifiers to combat abuse, the first key challenge is that:</p> <blockquote><p>Past training examples become obsolete as attacks evolve</p> </blockquote><p>While there are no silver bullet to deal with this obsolescence, here are three complementary strategies that helps coping with ever-changing data.</p> <h3>1. Automate model retraining</h3><p>You need to automate model retraining on fresh data so your model keeps up with the evolution of attacks. When you automate model retraining, it is a good practice to have a <a href="https://en.wikipedia.org/wiki/Training,_test,_and_validation_sets">validation set</a> that ensures the new model performs correctly and doesn’t introduce regressions. It is also useful to add <a href="https://en.wikipedia.org/wiki/Hyperparameter_optimization">hyperparameter optimization</a> to your retraining process to maximize your model accuracy.</p> <h3>2. Build highly generalizable models</h3><p>Your models have to be designed in a way that ensures they can generalize enough to detect new attacks. While ensuring that a model <a href="https://arxiv.org/pdf/1710.05468.pdf">generalizes well</a> is complex making sure you model have enough (but not too much) capacity (i.e., enough neurons) and quite a lot of training data is a good starting point.</p> <p> <img src="https://www.elie.net/static/images/images/challenges-faced-while-training-an-ai-to-combat-abuse/data-augmentation-impact.png" alt="Impact of data augmentation"/> </p> <p>If you don’t have enough real attack examples, you can supplement your training data with <a href="https://medium.com/nanonets/how-to-use-deep-learning-when-you-have-limited-data-part-2-data-augmentation-c26971dc8ced">data augmentation techniques</a> that increase the size of your corpus by generating slight variation of your attack examples. As visible in the table above, taken from <a href="https://arxiv.org/pdf/1608.06993.pdf">this paper</a> , data augmentation make models more robust and do increase accuracy significantly.</p> <p> <img src="https://www.elie.net/static/images/images/challenges-faced-while-training-an-ai-to-combat-abuse/learning-rates.jpg" alt="Learning rates"/> Finally you should consider other finer, well-documented technical aspects, such as <a href="http://cs231n.github.io/neural-networks-3/">tuning the learning rate</a> and <a href="http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf">using dropout</a> .</p> <h3>3. Set up monitoring and in-depth defense</h3><p>Finally, you have to assume your model will be bypassed at some point, so you need to build defense in depth to mitigate this issue. You also need to set up monitoring that will alert you when this occurs. Monitoring for a drop in the number of detected attacks or a spike in user reports is a good starting point.</p> <h3>Gmail malicious attacks</h3><p> <img src="https://www.elie.net/static/images/images/challenges-faced-while-training-an-ai-to-combat-abuse/gmail-malicious-attack-day-to-day.png" alt="Gmail malicious attacks"/> </p> <p>Quite often, I get asked how quickly attacks are evolving in practice. While I don’t have a general answer, here is a key statistic that I hope will convince you that attackers indeed mutate their attack incredibly quickly: 97 percent of Gmail malicious attachments blocked today are different from the ones blocked yesterday.</p> <p>Fortunately those new malicious attachments are variations of recent attacks and therefore can be blocked by systems that generalize well and are trained regularly.</p> <h2>Lack of ground truth data</h2><p> <img src="https://www.elie.net/static/images/images/challenges-faced-while-training-an-ai-to-combat-abuse/dog-vs-cat.jpg" alt="Dog and Cat"/> </p> <p>For most classification tasks, collecting training data is fairly easy because you can leverage human expertise. For example, if you want to build an animal classifier, you could ask people to take a picture of animals and tell you which animals are in it.</p> <p> <img src="https://www.elie.net/static/images/images/challenges-faced-while-training-an-ai-to-combat-abuse/play-reviews.jpg" alt="Play Store reviews"/> </p> <p>On the other hand, collecting ground truth (training data) for anti-abuse purposes is not that easy because bad actors try very hard to impersonate real users. As a result, it is very hard even for humans to tease apart what is real and what is fake. For instance, the screenshot above showcases two Play store reviews. Would you be able to tell me which one is real and which one is fake?</p> <p>Obviously telling them apart is impossible because they are both well written and over the top. This struggle to collect abusive content accurately exists all across the board whether it is for reviews, comments, fake accounts or network attacks. By the way, both reviews are real in case you were wondering.☺️</p> <p>Accordingly, the second challenge on the quest to train a successful classifier is that:</p> <blockquote><p>Abusers try to hide their activities, which makes it hard to collect ground truth data</p> </blockquote><p>While no definitive answers exist on how to overcome this challenge, here are three techniques to collect ground truth data that can help alleviate the issue.</p> <h3>1. Applying clustering methods</h3><p>First, you can leverage <a href="https://en.wikipedia.org/wiki/Cluster_analysis">clustering methods</a> to <a href="https://arxiv.org/abs/1512.05457">expand upon known abusive content</a> to find more of it. It is often hard to find the right balance while doing so because if you are clustering too much, you end up flagging good content as bad, and if you don’t cluster enough, you won’t collect enough data.</p> <h3>2. Collecting ground truth with honeypots</h3><p> <a href="https://bit.ly/1GRILUE">Honeypots</a> -controlled settings ensure you that they will only collect attacks. The main difficulty with honeypots is to make sure that the collected data is representative of the set of the attacks experienced by production systems. Overall, honeypots are very valuable, but it takes a significant investment to get them to collect meaningful attacks.</p> <h3>3. Leverage generative adversarial networks</h3><p> <img src="https://www.elie.net/static/images/images/challenges-faced-while-training-an-ai-to-combat-abuse/dgan-examples.jpg" alt="Examples of Data augmentation using GAN"/> </p> <p>A new and promising direction is to leverage the recent advance in machine learning and use a <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network">Generative Adversarial Network</a> (main paper), better known as GAN, to <a href="https://arxiv.org/abs/1711.04340">reliably increase your training dataset</a> . The screenshot above, taken from this paper, show you an example of face generation using it: only the top left image is real. While still very experimental, here is one of the last paper on the topic, this approach is exciting as it paves the way to generate meaningful attack variations at scale.</p> <h2>Ambiguous data &amp; taxonomy</h2><p>The third challenge that arises when building a classifier is that what we consider bad is often ill defined, and there are a lot of borderline cases where even humans struggle to make a decision.</p> <p> <img src="https://www.elie.net/static/images/images/challenges-faced-while-training-an-ai-to-combat-abuse/context-matters.jpg" alt="Context matters"/> </p> <p>For example, the sentence “I am going to kill you” can either be viewed as the sign of a healthy competition if you are playing a video game with your buddies or it can be a threat if it is used in a serious argument. More generally, it is important to realize that:</p> <blockquote><p>Unwanted content is inherently context, culture and settings dependent</p> </blockquote><p>Accordingly, is it impossible, except for very specific use cases such as profanity or gibberish detection, to build universal classifiers that will work across all products and for all users.</p> <p> <img src="https://www.elie.net/static/images/images/challenges-faced-while-training-an-ai-to-combat-abuse/spam-foldering.jpg" alt="Spam foldering"/> </p> <p>When you think about it, even the well-established concept of SPAM is ill defined and means different things for different people. For example, countless Gmail users decide that the emails coming from a mailing list they willingly subscribed to a long time ago are now spam because they lost interest in the topic.</p> <p>Here are three way to help your classifier deal with ambiguity:</p> <ol> <li><p><strong>Model context, culture and settings</strong>: Easier said than done! Add features that represent the context in which the classification is performed. This will ensure that the classifier is able to reach a different decision when the same data is used in different settings.</p> </li> <li><p><strong>Use personalized models</strong>: Your models need to be architectured in a way that takes into account user interests and levels of tolerance. This can be done by adding some features (pioneer paper) that model user behavior.</p> </li> <li><p><strong>Offer users additional meaningful choices</strong>: You can reduce ambiguity by providing users with alternative choices that are more meaningful than a generic reporting mechanism. Those more precise choices reduce ambiguity by reducing the number of use cases that are clamped behind a single ill-defined concept, such as spam.</p> </li> </ol> <p> <img src="https://www.elie.net/static/images/images/challenges-faced-while-training-an-ai-to-combat-abuse/gmail-blocking-option.png" alt="Gmail blocking option"/> </p> <p>Here is a concrete example of how the addition of meaningful choices reduces ambiguity. Back in 2015, Gmail started offering its users the ability to easily unsubscribe from mailing lists and block senders, giving them more control over their inboxes. Under the hood, this new options helps the classifiers as they reduce the ambiguity of what is marked as spam.</p> <h2>Lack of obvious features</h2><p>Our fourth and last training challenge is that some products lack obvious features. Until now, we have focused on classifying rich content such as text, binary and image, but not every product has such rich content.</p> <p> <img src="https://www.elie.net/static/images/images/challenges-faced-while-training-an-ai-to-combat-abuse/youtube-views.jpg" alt="Youtube views"/> </p> <p>For example, Youtube has to be defended against fake views, and not a lot of obvious features that can be leveraged to do so. Looking at the view count timeline for the famous Gangnam style video, you will notice two anomalous peaks. These might be from spammers or simply because the video had huge spikes due to virality. It is impossible to tell by just looking at how the view count grew over time.</p> <p>In general, AI thrives on feature-rich problems such as text or image classification; however, abuse fighters have to make AI work across the board to protect all users and products. This need to cover the entire attack surface led us to use AI to tackle use-cases that are less and ideal, and sooner or later we have to face a hard truth:</p> <blockquote><p>Some products in need of protection don’t have the rich features AI thrives on</p> </blockquote><p>Fortunately, you can (partially) work around the lack of rich features. In a nutshell, the way to build an accurate classifier when you don’t have enough content features is to leverage auxiliary data as much as possible. Here are three key sources of auxiliary data you can potentially use:</p> <ol> <li><p><strong>Context</strong>: Everything related to the client software or network can be used, including the user agent, the client IP address and the screen resolution.</p> </li> <li><p><strong>Temporal behavior</strong>: Instead of looking at an event in isolation, you can model the sequence of actions that is generated by each user. You can also look at the sequence of actions that target a specific artifact, such as a given video. Those temporal sequences provide a rich set of statistical features.</p> </li> <li><p><strong>Anomaly detection</strong>: It is impossible for an attacker to fully behave like a normal user, so anomaly features can almost always be used to boost detection accuracy.</p> </li> </ol> <p>The last point is not as obvious as it seems so let’s deep dive into it.</p> <p> <img src="https://www.elie.net/static/images/images/challenges-faced-while-training-an-ai-to-combat-abuse/various-attackers.png" alt="Attackers"/> </p> <p>At its core, what separates rudimentary attackers from advanced ones is their ability to accurately impersonate legitimate user behavior. However, because attackers aim at gaming the system, there always will be some behaviors that they can’t spoof.</p> <p>It is those unspoofable behaviors that we aim at detecting using <a href="https://en.wikipedia.org/wiki/One-class_classification">one-class classification</a> . Introduced <a href="http://homepage.tudelft.nl/n9d04/thesis.pdf">circa 1996</a> , the idea behind one-class classification is to use AI to find all the entities belonging to a single class (the normal behavior in our case) out of all entities that exist in a dataset. Every entity that is not member of that class is then considered an outlier.</p> <p> <img src="https://www.elie.net/static/images/images/challenges-faced-while-training-an-ai-to-combat-abuse/one-class-classification-example.jpg" alt="One class classifier"/> </p> <p>For abuse purposes, one-class classification allows to detect anomaly/potential attacks even when you have no attack examples. For example, the figure above shows in red a set of malicious IPs attacking Google products that were detected using this type of approach.</p> <p>Overall, one-class classification is a great complement to more traditional AI systems as its requirements are fundamentally different. As mentioned earlier, you can even take this one step further and feed the result of your one-class classifier to a standard one (binary class) to boost its accuracy.</p> <p>This wraps up our deep dive into the challenges faced while training an anti-abuse classifier. The <a src="https://www.elie.net/blog/ai/how-to-handle-mistakes-while-using-ai-to-block-attacks">next post</a> covers the challenges that arise when you start running your classifier in production. The <a src="https://www.elie.net/blog/ai/attacks-against-machine-learning-an-overview">final post</a> of the serie discusses the various attacks against classifiers and how to mitigate them</p> <p>Thank you for reading this post till the end! If you enjoyed it, don’t forget to share it on your favorite social network so that your friends and colleagues can enjoy it too and learn about AI and anti-abuse.</p> <p>To get notified when my next post is online, follow me on <a href="https://twitter.com/elie">Twitter</a> , <a href="https://www.facebook.com/elieblog">Facebook</a> , <a href="https://plus.google.com/+ElieBursztein">Google+</a> , or <a href="https://www.linkedin.com/in/bursztein/">LinkedIn</a> . You can also get the full posts directly in your inbox by subscribing to the mailing list or via <a href="http://feeds.feedburner.com/ebursztein">RSS</a> .</p> <p>A bientôt!</p>
]]>
</content>
<link type="text/html" rel="alternate" href="https://www.elie.net/blog/ai/challenges-faced-while-training-an-ai-to-combat-abuse?utm_source=rss"/>
<author>
<name>Elie Bursztein</name>
<email>feed@elie.net</email>
</author>
</entry>
<entry>
<published>2018-04-19T00:00:00Z</published>
<updated>2018-04-19T00:00:00Z</updated>
<id>
https://www.elie.net/talk/how-to-successfully-harness-ai-to-combat-fraud-and-abuse?utm_source=rss
</id>
<title>
How to successfully harness AI to combat fraud and abuse
</title>
<content type="html">
<![CDATA[
<p> <img src="https://www.elie.net/static/images/banner/how-to-successfully-harness-ai-to-combat-fraud-and-abuse.png"/> </p> <p>While machine learning is integral to innumerable anti-abuse systems including spam and phishing detection, the road to reap its benefits is paved with numerous abuse-specific challenges. Drawing from concrete examples this session will discuss how these challenges are addressed at Google and providea roadmap to anyone interested in applying machine learning to fraud and abuse problems. Watching this talk will allow you to:</p> <ol> <li>Learn how machine learning helps combat fraud and abuse.</li> <li>Discover how to overcome challenges faced when using machine learning to anti-abuse.</li> <li>Understand what the unsolved challenges are in the space.</li> </ol>
]]>
</content>
<link type="text/html" rel="alternate" href="https://www.elie.net/talk/how-to-successfully-harness-ai-to-combat-fraud-and-abuse?utm_source=rss"/>
<author>
<name>Elie Bursztein</name>
<email>feed@elie.net</email>
</author>
</entry>
<entry>
<published>2018-04-19T00:00:00Z</published>
<updated>2018-05-29T07:00:00Z</updated>
<id>
https://www.elie.net/blog/ai/harnessing-ai-to-combat-fraud-and-abuse-ai-is-the-key-to-robust-defenses?utm_source=rss
</id>
<title>Why AI is the key to robust anti-abuse defenses</title>
<content type="html">
<![CDATA[
<p> <img src="https://www.elie.net/static/images/banner/harnessing-ai-to-combat-fraud-and-abuse-ai-is-the-key-to-robust-defenses.jpg"/> </p> <p>This post explains why artificial intelligence (AI) is the key to building anti-abuse defenses that keep up with user expectations and combat increasingly sophisticated attacks. This is the first post of a series of four posts dedicated to provide a concise overview of how to harness AI to build robust anti-abuse protections.</p> <p>The remaining three posts delve into the top 10 anti-abuse specific challenges encountered while applying AI to abuse fighting, and how to overcome them. Following the natural progression of building and launching an AI-based defense system, <a src="https://www.elie.net/blog/ai/challenges-faced-while-training-an-ai-to-combat-abuse">the second post</a> covers the challenges related to training, the <a src="https://www.elie.net/blog/ai/how-to-handle-mistakes-while-using-ai-to-block-attacks">third post</a> delves into classification issues and the <a src="https://www.elie.net/blog/ai/attacks-against-machine-learning-an-overview">fourth and final post</a> looks at how attackers go about attacking AI-based defenses.</p> <p>This series of posts is modeled after the talk I gave at <a src="https://www.elie.net/talk/how-to-successfully-harness-ai-to-combat-fraud-and-abuse/">RSA 2018</a> . Here is a re-recording of this talk:</p> <p> <div class="video"> <iframe width="560" height="315" src="https://www.youtube.com/embed/5gxI-6QmPdE" frameborder="0" allowfullscreen></iframe> </div> </p> <p>You can also get the <a src="https://www.elie.net/talk/how-to-successfully-harness-ai-to-combat-fraud-and-abuse/">slides here</a> .</p> <p><strong>Disclaimer</strong>: This series is meant to provide an overview for everyone interested in the subject of harnessing AI for anti-abuse defense, and it is a potential blueprint for those who are making the jump. Accordingly, this series focuses on providing a clear high-level summary, purposely avoiding delving into technical details. That being said, if you are an expert, I am sure you will find ideas and techniques that you haven’t heard about before, and hopefully you will be inspired to explore them further.</p> <p>Let’s kickoff this series with a motivating example.</p> <p> <img src="https://www.elie.net/static/images/images/harnessing-ai-to-combat-fraud-and-abuse-ai-is-the-key-to-robust-defenses/nyt-trolls.png" alt="NYT trolls"/> </p> <p>I am an avid reader of <em>The New York Times</em>, and one of my favorite moments on the site is when I find a comment that offers an insightful perspective that helps me better understand the significance of the news reported. Knowing this, you can imagine my unhappiness, back in September 2017, when <em>The New York Times</em> <a href="https://www.nytimes.com/2017/09/27/reader-center/comments-moderation.html">announced</a> its decision to close the comment sections because it couldn't keep up with the trolls that rentelessy attempted to derail the conversation :-(</p> <p> <img src="https://www.elie.net/static/images/images/harnessing-ai-to-combat-fraud-and-abuse-ai-is-the-key-to-robust-defenses/nyt-close-comments.png" alt="NYT closes comments"/> </p> <p>This difficult decision created a backlash from their readership, which felt censored and didn’t understand the reasoning behind it. This led <em>The New York Times</em> to <a href="https://www.nytimes.com/2017/09/27/reader-center/comments-moderation.html">go on record</a> a few days after to explain that it couldn’t keep up with the troll onslaught and felt it had no other choice than closing the comments, in order to maintain the quality of the publication.</p> <h2>Conventional protections are failing</h2><p><em>The New York Times</em> case is hardly an exception. <a href="https://www.wired.com/2015/10/brief-history-of-the-demise-of-the-comments-timeline/">Many other publications</a> have disallowed comments due to trolling. More generally, many online services, including games and recommendation services, are struggling to keep up with the continuous onslaught of abusive attempts. These struggles are the symptom of a larger issue:</p> <blockquote><p>Conventional abuse defenses are falling behind</p> </blockquote><p> <img src="https://www.elie.net/static/images/images/harnessing-ai-to-combat-fraud-and-abuse-ai-is-the-key-to-robust-defenses/abuse-conventional-defense-failing-underlying-factor.png" alt="Conventional Abuse defense failing"/> </p> <p>Three major underlying factors contribute to the failure of conventional protections:</p> <ol> <li><strong>User expectations and standards have dramatically increased.</strong> These days, users perceive the mere presence of a single abusive comment, spam email or bad images as a failure of the system to protect them.</li> <li><strong>The amount and diversity of user-generated content has exploded.</strong> Dealing with this explosion requires anti-abuse systems to scale up to cover a large volume of diverse content and a wide range of attacks.</li> <li><strong>Attacks have become increasingly sophisticated.</strong> Attackers never stop evolving, and online services are now facing well-executed, coordinated attacks that systematically attempt to target their defense’s weakest points.</li> </ol> <h2>AI is the way forward</h2><p> <img src="https://www.elie.net/static/images/images/harnessing-ai-to-combat-fraud-and-abuse-ai-is-the-key-to-robust-defenses/ai-based-defense.png" alt="AI based defense"/> </p> <p>So, if conventional approaches are failing, how do we build anti-abuse protection that is able to keep up with those ever-expanding underlying factors? Based on our experience at Google, I argue that:</p> <blockquote><p>AI is key to building protections that keep up with user expectations and combat increasingly sophisticated attacks.</p> </blockquote><p> <img src="https://www.elie.net/static/images/images/harnessing-ai-to-combat-fraud-and-abuse-ai-is-the-key-to-robust-defenses/ai-really.png" alt="AI really"/> </p> <p>I know! The word AI is thrown around a lot these days, and skepticism surrounds it. However, as I am going to explain, there are fundamental reasons why AI is currently the best technology to build effective anti-fraud and abuse protections.</p> <h2>AI to the rescue of <em>The NYT</em></h2><p>Before delving into those fundamental reasons, let’s go back to The New York Times story so I can tell you how it ended.</p> <p> <img src="https://www.elie.net/static/images/images/harnessing-ai-to-combat-fraud-and-abuse-ai-is-the-key-to-robust-defenses/nyt-reopen-comments.png" alt="NYT reopening comments"/> </p> <p><em>The New York Times</em> story has <a href="https://www.nytimes.com/2017/06/13/insider/have-a-comment-leave-a-comment.html">an awesome ending</a> : not only were the comments reopened, but they were also extended to many more articles.</p> <p>What made this happy ending possible, under the hood, is an AI system developed by Google and Jigsaw that empowered <em>The NYT</em> to scale up its comment moderation.</p> <p> <img src="https://www.elie.net/static/images/images/harnessing-ai-to-combat-fraud-and-abuse-ai-is-the-key-to-robust-defenses/perspective-api.jpg" alt="Perspective API"/> </p> <p>This system, called <a href="https://www.perspectiveapi.com/">Perspective API</a> , leverages deep learning to assign a toxicity score to the 11,000 comments posted daily on The New York Times site. The NYT comments review team leverages those scores to scale up by only focusing on the potentially toxic comments. Since its release, many websites have adopted Perspective API, including <a href="https://medium.com/jigsaw/algorithms-and-insults-scaling-up-our-understanding-of-harassment-on-wikipedia-6cc417b9f7ff">Wikipedia</a> and <a href="https://www.wired.com/2017/02/googles-troll-fighting-ai-now-belongs-world/">The Guardian</a> .</p> <h2>The fundamental reasons behind the ability of AI to combat abuse</h2><p> <img src="https://www.elie.net/static/images/images/harnessing-ai-to-combat-fraud-and-abuse-ai-is-the-key-to-robust-defenses/ai-for-anti-abuse-fundamental-reason.jpg" alt="AI for anti abuse reasons"/> </p> <p>Fundamentally, AI allows to build robust abuse protections because it is able to do the following better than any other systems:</p> <ol> <li><strong>Data generalization</strong>: Classifiers are able to accurately block content that matches ill-defined concepts, such as spam, by generalizing efficiently from their training examples.</li> <li><strong>Temporal extrapolation</strong>: AI systems are able to identify new attacks based on the ones observed previously.</li> <li><strong>Data maximization</strong>: By nature, an AI is able to optimally combine all the detection signals to come up with the best decision possible. In particular, it is able to exploit the nonlinear relations that exist between the various data inputs.</li> </ol> <p> <img src="https://www.elie.net/static/images/images/harnessing-ai-to-combat-fraud-and-abuse-ai-is-the-key-to-robust-defenses/deep-learning-scale-with-data.jpg" alt="Deep learning scales with data"/> </p> <p>The final piece of the puzzle that explains why AI is overtaking anti-abuse fighting, and many other fields, is the rise of deep learning. What makes deep learning so powerful is that deep neural networks, in contrast to previous AI algorithms, <a href="http://research.baidu.com/deep-learning-scaling-predictable-empirically/">scale up</a> as more data and computational resources are used.</p> <p> <img src="https://www.elie.net/static/images/images/harnessing-ai-to-combat-fraud-and-abuse-ai-is-the-key-to-robust-defenses/deep-learning-impact-on-abuse-fighthing.png" alt="Deep learning impact on anti abuse fighting"/> </p> <p>From an abuse-fighting perspective, this ability to scale up is a game changer because it moves us from a world where more data means more problems to a world where more data means better defense for users.</p> <h2>How deep learning helps Gmail to stay ahead of spammers</h2><p> <img src="https://www.elie.net/static/images/images/harnessing-ai-to-combat-fraud-and-abuse-ai-is-the-key-to-robust-defenses/gmail-attack-surface.jpg" alt="Gmail attack surface"/> </p> <p>Every week, Gmail’s anti-spam filter automatically scans <a href="https://elie.net/talk/targeted-attacks-against-corporate-inboxes-a-gmail-perspective">hundred of billions of emails</a> to protect its billion-plus users from phishing, spam and malware.</p> <p> <img src="https://www.elie.net/static/images/images/harnessing-ai-to-combat-fraud-and-abuse-ai-is-the-key-to-robust-defenses/gmail-filter-breakdown.jpg" alt="Gmail filters breakdown"/> </p> <p>The component that keeps Gmail’s filter ahead of spammers is its deep learning classifier. The 3.5% additional coverage it provides comes mostly from its ability to the detect advanced spam and phishing attacks that are missed by the other part of the filter, including the previous generation linear classifier.</p> <h2>Deep learning is for everyone</h2><p> <img src="https://www.elie.net/static/images/images/harnessing-ai-to-combat-fraud-and-abuse-ai-is-the-key-to-robust-defenses/ai-really.png" alt="AI really"/> </p> <p>Now, some of you might think that deep learning only works for big companies like Google or that it is still very experimental or too expensive. Nothing could be further from the truth.</p> <p>Over the last three years, deep learning has become very mature. Between Cloud APIs and free frameworks, it is very easy and quick to start benefiting from deep learning. For example, <a href="https://www.tensorflow.org/">Tensorflow</a> and <a href="https://keras.io/">Keras</a> provide a very performant, robust and well-documented framework that empowers you to build state-of-the-art classifiers with just a few lines of code. You can find <a href="https://github.com/keras-team/keras/tree/master/examples">pre-trained models here</a> , a list of keras related ressources <a href="https://github.com/fchollet/keras-resources">here</a> and one for Tensorflow <a href="https://github.com/jtoy/awesome-tensorflow">here</a> .</p> <h2>Challenges ahead</h2><p> <img src="https://www.elie.net/static/images/images/harnessing-ai-to-combat-fraud-and-abuse-ai-is-the-key-to-robust-defenses/ai-challenges-ahead.png" alt="AI challenges"/> </p> <p>While it is clear that AI is the way forward to build robust defenses, this does not mean that the road to success is without challenges. The next three posts will delve into the top 10 anti-abuse specific challenges encountered while applying AI to abuse fighting, and how to overcome them.</p> <p> <img src="https://www.elie.net/static/images/images/harnessing-ai-to-combat-fraud-and-abuse-ai-is-the-key-to-robust-defenses/how-to-apply-ai-to-anti-abuse-roadmap.png" alt="How to apply AI to anti abuse roadmap"/> </p> <p>Those 10 challenges are grouped into the following three categories/posts that follow the natural progression of building and launching an AI-based defense system:</p> <ol> <li><strong>Training</strong>: <a src="https://www.elie.net/blog/ai/challenges-faced-while-training-an-ai-to-combat-abuse">This post</a> looks at how to overcome the four main challenges faced while training anti-abuse classifiers, as those challenges are the ones you will encounter first.</li> <li><strong>Classification</strong>: <a src="https://www.elie.net/blog/ai/how-to-handle-mistakes-while-using-ai-to-block-attacks">This post</a> delves into the two key problems that arise when you put your classifier in production and start blocking attacks.</li> <li><strong>Attacks</strong>: <a src="https://www.elie.net/blog/ai/attacks-against-machine-learning-an-overview">The last post</a> of the series discusses the four main ways attackers try to derail classifiers and how to mitigate them.</li> </ol> <p>Thank you for reading this post till the end! If you enjoyed it, don’t forget to share it on your favorite social network so that your friends and colleagues can enjoy it too and learn about AI and anti-abuse.</p> <p>To get notified when my next post is online, follow me on <a href="https://twitter.com/elie">Twitter</a> , <a href="https://www.facebook.com/elieblog">Facebook</a> , <a href="https://plus.google.com/+ElieBursztein">Google+</a> , or <a href="https://www.linkedin.com/in/bursztein/">LinkedIn</a> . You can also get the full posts directly in your inbox by subscribing to the mailing list or via <a href="http://feeds.feedburner.com/ebursztein">RSS</a> .</p> <p>A bientôt!</p>
]]>
</content>
<link type="text/html" rel="alternate" href="https://www.elie.net/blog/ai/harnessing-ai-to-combat-fraud-and-abuse-ai-is-the-key-to-robust-defenses?utm_source=rss"/>
<author>
<name>Elie Bursztein</name>
<email>feed@elie.net</email>
</author>
</entry>
<entry>
<published>2018-03-25T00:00:00Z</published>
<updated>2017-03-25T00:00:00Z</updated>
<id>
https://www.elie.net/blog/security/taking-down-gooligan-a-retrospective-analysis-part-3-monetization-and-clean-up?utm_source=rss
</id>
<title>
Taking down Gooligan part 3 — monetization and clean-up
</title>
<content type="html">
<![CDATA[
<p> <img src="https://www.elie.net/static/images/banner/taking-down-gooligan-a-retrospective-analysis-part-3-monetization-and-clean-up.jpg"/> </p> <p>This post provides an in-depth analysis of Gooligan monetization schemas and recounts how Google took it down with the help of external partners.</p> <p>This post is the final post of the series dedicated to the hunt and take down of Gooligan that we did at Google in collaboration with Check Point in November 2016. The <a src="https://www.elie.net/blog/security/taking-down-gooligan-part-1-overview">first post</a> recounts the Gooligan origin story and offers an overview of how it works. The <a src="https://www.elie.net/blog/security/taking-down-gooligan-part-2-inner-working">second one</a> provides an in-depth analysis of Gooligan’s inner workings and an analysis of its network infrastructure. As this post builds on the previous two, I encourage you to read them if you haven’t done so already.</p> <p>This series of posts is modeled after the talk I gave on the subject at Botconf in December 2017. Here is a recording of the talk:</p> <p> <div class="video"> <iframe width="560" height="315" src="https://www.youtube.com/embed/gt1loXpk-0A" frameborder="0" allowfullscreen></iframe> </div> </p> <p>You can also get the slides <a src="https://www.elie.net/talk/hunting-down-gooligan-a-retrospective-analysis">here</a> , but they are pretty bare.</p> <h2>Monetization</h2><p>Gooligan’s goal was to monetize the infected devices through two main fraudulent schemas: Ad fraud and Android app boosting.</p> <h3>Ad fraud</h3><p> <img src="https://www.elie.net/static/images/images/taking-down-gooligan-a-retrospective-analysis-part-3-monetization-and-clean-up/gooligan-fraudulent-ads-popup-example.jpg" alt="Gooligan Fraudulent ads pop up"/> </p> <p>As shown in the screenshot above, periodically Gooligan will use its root privileges to overlay an ad popup for a legitimate app on top of any activity the user was currently doing. Under the hood, Gooligan knows when the user is looking at the phone, as it monitors various key events, including when the screen is turned on.</p> <p>We don’t have much insight on how effective those ad campaigns were or who was reselling them, as they don’t abuse Google’s ads network, and they use a gazillion HTTP redirects, which makes attribution close to impossible. However we believe that ad fraud was the main driver of Gooligan revenue, given its volume and the fact that we blocked its fake installs as discussed below.</p> <h3>App Boosting</h3><p>The second way Gooligan attempted to monetize infected devices was by performing Android app boosting. An app boosting package is a bundle of searches for a specific query on the Play store, followed by an install and a review. The search is used in an attempt to rank the app for a given term. This tactic is commonly peddled in App Store Optimization (ASO) guides.</p> <p> <img src="https://www.elie.net/static/images/images/taking-down-gooligan-a-retrospective-analysis-part-3-monetization-and-clean-up/example-of-play-boosting-service.jpg" alt="Example of Play Store boosting service"/> </p> <p>The reason Gooligan went through the trouble of stealing OAuth tokens and manipulating the Play store is probably that the defenses we put in place are very effective at detecting and discounting fake synthetic installs. Using real devices with real accounts was the Gooligan authors’ attempt to evade our detection systems. Overall, it was a total failure on their side: We caught all the fake installs, and suspended the abusive apps and developers.</p> <p> <img src="https://www.elie.net/static/images/images/taking-down-gooligan-a-retrospective-analysis-part-3-monetization-and-clean-up/gooligan-play-store-fraud-diagram.png" alt="Play Store Fraud Diagram"/> </p> <p>As illustrated in the diagram above, the app boosting was done in four steps:</p> <ol> <li><p><strong>Token stealing</strong>: The malware extracts the phone’s long term token from the phone’s accounts.</p> </li> <li><p><strong>Taking order</strong>: Gooligan reports phone information to the central command and control system, and receives in response a reply telling it which app to boost, including which search term to use and which comment to leave (if any). Phone information is exfiltrated because Gooligan authors also had access to non-compromised phones and were trying to use information obtained from Gooligan to fake requests from those phones.</p> </li> <li><p><strong>Token exchange</strong>: The long term token is exchanged for a short term token that allows Gooligan to access the Play store. We are positive that no user data was compromised by Gooligan, as no other data was ever requested by Gooligan.</p> </li> <li><p><strong>Boosting</strong>: The fake search, installation, and potential review is carried out through the manipulated Play store app.</p> </li> </ol> <h2>Clean-up</h2><p>Cleaning up Gooligan was challenging for two reasons: First, as discussed in the <a href="https://elie.net/blog/security/taking-down-gooligan-part-1-overview">infection post</a> , its reset persistence mechanism meant that doing a factory reset was not enough to clean up the old unpatched devices. Second, the Oauth tokens had been exfiltrated to Gooligan servers.</p> <p>Asking users to reflash their devices would have been unreasonable and issuing an OTA (Over The Air) update would have take too long. Given this difficult context and the need to act quickly to protect our users we went for an alternative solution that we rarely use: orchestrating a takedown with the help of third parties.</p> <h3>Takedown</h3><p> <img src="https://www.elie.net/static/images/images/taking-down-gooligan-a-retrospective-analysis-part-3-monetization-and-clean-up/gooligan-sinkhole-efficiency.jpg" alt="Gooligan sinkhole efficiency chart"/> </p> <p>With the help of Shadowserver foundation and domain registrars we sinkholed Gooligan domains and got them to point to Shadowserver controlled IPs instead of IPs controlled by Gooligan authors. This sinkholing ensured that infected devices couldn’t exfiltrate token or receive fraud commands, as they would connect to sinkhole servers instead of the real command and control servers. As shown in the graph above, our takedown was very successful: It blocked over 50M attempts to connect to Gooligan’s control server in 2017.</p> <h3>Notifications</h3><p> <img src="https://www.elie.net/static/images/images/taking-down-gooligan-a-retrospective-analysis-part-3-monetization-and-clean-up/notification-sent-to-gooligan-victims-example.jpg" alt="Example of Notifications sent to Gooligan victims"/> </p> <p>With the sinkhole in place, the second part of the remediation involved resecuring the accounts that were compromised, by disabling the exfiltrated tokens and notifying the users. Notification at that scale is very complex, for three key reasons:</p> <ul> <li><p>Reaching users in a timely fashion across a wide range of devices is difficult. We ended up using a combination of SMS, email, and Android messaging, depending on what communication channel was available.</p> </li> <li><p>It was important to make the notification understandable and useful to all users. Explaining what was happening clearly and simply took a lot of iteration. We ended up with the notification shown in the screenshot above.</p> </li> <li><p>Once crafted, the text of the notification and help page had to be translated into the languages spoken by our users. Performing high quality internationalization for over 20 languages very quickly was quite a feat.</p> </li> </ul> <h2>Epilogue</h2><p>Overall, in order to respond to Gooligan, many people, including myself, ended up working long hours through the Thanksgiving weekend (an important holiday in the U.S.). Our commitment to quickly eradicate this threat paid off: On the evening of Monday, November 29th, the takedown took place, followed the next day by the resecuring of the compromised accounts. All in all, this takedown took a mere few days, which is blazing fast when you compare it to other similar ones. For example, the <a href="https://en.wikipedia.org/wiki/Avalanche_(phishing_group">Avalanche botnet</a> ) takedown took four years of intensive efforts.</p> <p>To conclude, Gooligan was a very challenging malware to tackle, due to its scale and unconventional tactics. We were able to meet this challenge and defeat it, thanks to a cross-industry effort and the involvement of many teams at Google that didn’t go home until users were safe.</p> <p>Thanks for reading this post all the way to the end. I hope it showcases how we approach botnet fighting and sheds some light on some of the lesser known, yet still critical, activities that our research team assists with.</p> <p>Thank you for reading this post till the end! If you enjoyed it, don’t forget to share it on your favorite social network so that your friends and colleagues can enjoy it too and learn about Gooligan.</p> <p>To get notified when my next post is online, follow me on <a href="https://twitter.com/elie">Twitter</a> , <a href="https://www.facebook.com/elieblog">Facebook</a> , <a href="https://plus.google.com/+ElieBursztein">Google+</a> , or <a href="https://www.linkedin.com/in/bursztein/">LinkedIn</a> . You can also get the full posts directly in your inbox by subscribing to the mailing list or via <a href="http://feeds.feedburner.com/ebursztein">RSS</a> .</p> <p>A bientôt!</p>
]]>
</content>
<link type="text/html" rel="alternate" href="https://www.elie.net/blog/security/taking-down-gooligan-a-retrospective-analysis-part-3-monetization-and-clean-up?utm_source=rss"/>
<author>
<name>Elie Bursztein</name>
<email>feed@elie.net</email>
</author>
</entry>
<entry>
<published>2018-03-18T00:00:00Z</published>
<updated>2017-03-18T00:00:00Z</updated>
<id>
https://www.elie.net/blog/security/taking-down-gooligan-part-2-inner-working?utm_source=rss
</id>
<title>Taking down Gooligan: part 2 — inner workings</title>
<content type="html">
<![CDATA[
<p> <img src="https://www.elie.net/static/images/banner/taking-down-gooligan-part-2-inner-working.jpg"/> </p> <p>This post provides an in-depth analysis of the inner workings of Gooligan, the infamous Android OAuth stealing botnet.</p> <p>This is the second post of a series dedicated to the hunt and takedown of Gooligan that we did at Google, in collaboration with Check Point, in November 2016. The <a src="https://www.elie.net/blog/security/taking-down-gooligan-part-1-overview">first post</a> recounts Gooligan’s origin story and provides an overview of how it works. The <a src="https://www.elie.net/blog/security/taking-down-gooligan-a-retrospective-analysis-part-3-monetization-and-clean-up">final post</a> discusses Gooligan’s various monetization schemas and its take down. As this post builds on the <a src="https://www.elie.net/blog/security/taking-down-gooligan-part-1-overview">previous one</a> , I encourage you to read it, if you haven’t done so already.</p> <p>This series of posts is modeled after the talk I gave at <a href="https://www.botconf.eu/botconf-2017/">Botconf</a> in December 2017. Here is a re-recording of the talk:</p> <p> <div class="video"> <iframe width="560" height="315" src="https://www.youtube.com/embed/gt1loXpk-0A" frameborder="0" allowfullscreen></iframe> </div> </p> <p>You can also get the slides <a src="https://www.elie.net/talk/hunting-down-gooligan-a-retrospective-analysis">here</a> but they are pretty bare.</p> <h2>Infection</h2><p>Initially, users are tricked into installing Gooligan’s staging app on their device under one false pretense or another. Once this app is executed, it will fully compromise the device by performing the five steps outlined in the diagram below:</p> <p> <img src="https://www.elie.net/static/images/images/taking-down-gooligan-part-2-inner-working/gooligan-infection-process.jpg" alt="Googligan infection process"/> </p> <p>As emphasized in the chart above, the first four stages are mostly borrowed from <a href="https://en.wikipedia.org/wiki/Ghost_Push">Ghost Push</a> . Gooligan authors main addition is the code needed to instrument the Play Store app using a complex injection process. This heavy code reuse initially made it difficult for us to separate Ghost Push samples from Gooligan ones. However, as soon as we had the full kill chain analyzed, we were able to write accurate detection signatures.</p> <h3>Payload decoding</h3><p>Most Gooligan samples hide their malicious payload in a fake image located in <em>assets/close.png</em>. This file is encrypted with a hardcoded [XOR encryption] function. This encryption is used to escape the signatures that detect the code that Gooligan borrows from previous malware. Encrypting malicious payload is a very old malware trick that has been used by <a href="https://www.csc2.ncsu.edu/faculty/xjiang4/DroidKungFu.html">Android malware</a> since at least 2011.</p> <p> <img src="https://www.elie.net/static/images/images/taking-down-gooligan-part-2-inner-working/gooligan-initial-payload-file-structure.jpg" alt="Gooligan initial payload file structure"/> </p> <p>Besides its encryption function, one of the most prominent Gooligan quirks is its weird (and poor) integrity verification algorithm. Basically, the integrity of the <em>close.png</em> file is checked by ensuring that the first ten bytes match the last ten. As illustrated in the diagram above, the oddest part of this schema is that the first five bytes (<em>val 1</em>) are compared with the last five, while bytes six through ten (<em>val 2</em>) are compared with the first five.</p> <h3>Phone rooting</h3><p> <img src="https://www.elie.net/static/images/images/taking-down-gooligan-part-2-inner-working/kingroot-exploit-kit.jpg" alt="Gooligan initial payload file structure"/> </p> <p>As alluded to earlier, Gooligan, like Snappea and Ghostpush, weaponizes the <a href="https://kingroot.net/">Kingroot exploit kit</a> to gain root access. Kingroot operates in three stages: First, the malware gathers information about the phone that are sent to the exploit server. Next, the server looks up its database of exploits (which only affect Android 3.x and 4.x) and builds a payload tailored for the device. Finally, upon payload reception, the malware runs the payload to gain root access.</p> <p>The weaponization of known exploits by cyber-criminals who lack exploit development capacity (or don't want to invest into it) is as old as crimeware itself. For example, <a href="https://www.computerworld.com/article/2470698/mobile-apps/google-android-market-kills-droid-dream-malware-in-trojans.html">DroidDream</a> exploited <a href="https://thesnkchrmr.wordpress.com/2011/03/27/udev-exploit-exploid/">Exploid</a> and <a href="https://thesnkchrmr.wordpress.com/2011/03/24/rageagainstthecage/">RageAgainstTheCage</a> back in 2011. This pattern is common across every platform. For example, recently NSA-leaked exploit <a href="http://www.wired.co.uk/article/what-is-eternal-blue-exploit-vulnerability-patch">Eternal Blue was weaponized</a> by the fake ransomware NoPetya. If you are interested in ransomware actors, check <a src="https://www.elie.net/tag/ransomware">my posts</a> on the subject.</p> <h3>Persistence setup</h3><p>Upon rooting the device, Gooligan patches the <em>install-recovery.sh</em> script to ensure that it will survive a factory reset. This resilience mechanism was the most problematic aspect of Gooligan, from a remediation perspective, because for the oldest devices, it only left us with OTA (over the air) update and device re-flashing as a way to remove it. This situation was due to the fact that very old devices don't have <a href="https://source.android.com/security/verifiedboot/verified-boot">verified boot</a> , as it was introduced in Android 4.4.</p> <p> <img src="https://www.elie.net/static/images/images/taking-down-gooligan-part-2-inner-working/install-android-factory-image.jpg" alt="Android recovery"/> </p> <p>This difficult context, combined with the urgent need to help our users, led us to resort to a strategy that we rarely use: a coordinated takedown. The goal of this takedown was to disable key elements of the Gooligan infrastructure in a way that would ensure that the malware would be unable to work or update. As discussed in depth at the end of the post, we were able to isolate and take down Gooligan’s core server in less than a week thanks to a wide cross-industry effort. In particular, Kjell from the <a href="https://www.first.org/members/teams/norcert">NorCert</a> worked around the clock with us during the Thanksgiving holidays (thanks for all the help, Kjell!).</p> <h3>Play store app manipulation</h3><p>The final step of the infection is the injection of a shared library into the Play store app. This shared library allows Gooligan to manipulate the Play store app to download apps and inject review.</p> <p>We traced the injection code back to <a href="https://github.com/jekinleeph/LibInjectAll/blob/master/inject.c">publicly shared code</a> . The library itself is very bare: the authors added only the code needed to call Play store functions. All the fraud logic is in the main app, probably because the authors are more familiar with Java than C.</p> <h2>Impacted devices</h2><h3>Geo-distribution</h3><p> <img src="https://www.elie.net/static/images/images/taking-down-gooligan-part-2-inner-working/gooligan-geo-distribution.jpg" alt="Geo distribution of devices impacted by Gooligan"/> </p> <p>Looking at the set of devices infected during the takedown revealed that most of the affected devices were from India, Latin America, and Asia, as visible in the map above. 19% of the infections were from India, and the top eight countries affected by Gooligan accounted for more than 50% of the infections.</p> <h3>Make</h3><p> <img src="https://www.elie.net/static/images/images/taking-down-gooligan-part-2-inner-working/gooligan-infected-device-by-maker.jpg" alt="Phone maker distribution for devices impacted by Gooligan"/> </p> <p>In term of devices, as shown in the barchart above, the infections are spread across all the big brands, with Samsung and Micromax being unsurprisingly the most affected given their market share. Micromax is the leading Indian phone maker, which is not very well known in the U.S. and Europe because it has no presence there. It started manufacturing Android One devices in 2014 and is selling in quite a few countries besides India, most notably Russia.</p> <h2>Attribution</h2><h3>Initial clue</h3><p> <img src="https://www.elie.net/static/images/images/taking-down-gooligan-part-2-inner-working/haproxy-gooligan.jpg" alt="Gooligan HAproxy configuration"/> </p> <p>Buried deep inside Gooligan patient zero code, Check Point researchers <a href="https://www.linkedin.com/in/andrey-polkovnichenko-95670315/">Andrey Polkovnichenko</a> , <a href="https://www.linkedin.com/in/yoav-flint/">Yoav Flint Rosenfeld</a> , and <a href="https://www.linkedin.com/in/feixiang-asia/">Feixiang He</a> , who worked with us during the escalation, found the very unusual text string <em>oversea_adjust_read_redis</em>. This string led to the discovery of a Chinese blog post discussing load balancer configuration, which in turn led to the full configuration file of Gooligan backend services.</p> <pre><code><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1 2 3 4 5 6 7 8 9 10 11 12 13 14</pre></div></td><td class="code"><div class="highlight"><pre><span></span>#Ads API acl is_ads path_beg /overseaads/ use_backend overseaads if is_ads … #Payment API acl is_paystatis path_beg /overseapay/admin/ use_backend overseapaystatis if is_paystatis ... # Play install acl is_appstore path_beg /appstore/ use_backend overseapaystatis if is_appstore ... </pre></div> </td></tr></table></code></pre> <p>Analyzing the exposed <a href="http://www.haproxy.org/">HAproxy</a> configuration allowed us to pinpoint where the infrastructure was located and how the backend services were structured. As shown in the annotated configuration snippet above, the backend had API for click fraud, receiving payment from clients, and Play store abuse. While not visible above, there was also a complex admin and statistic-related API.</p> <h3>Infrastructure</h3><p> <img src="https://www.elie.net/static/images/images/taking-down-gooligan-part-2-inner-working/gooligan-infrastructure.jpg" alt="Gooligan infrastructure"/> </p> <p>Combining the API endpoints and IPs exposed in the HAproxy configuration with our knowledge of Gooligan binary allowed us to reconstruct the infrastructure charted above. Overall, Gooligan was split into two main data centers: one in China and one overseas in the US, which was using Amazon AWS IPs. After the takedown, all the infrastructure ended up moving back to China.</p> <p>Note: in the above diagram, the Fraud end-point appears twice. This is not a mistake: at Gooligan peak, its authors splited it out to sustain the load and better distribute the requests.</p> <h3>Actor</h3><p>So, who is behind Gooligan? Based on this infrastructure analysis and other data, we strongly believe that it is a group operating from mainland China. Publicly, the group claims to be a marketing company, while under the hood it is mostly focused on running various fraudulent schema. The apparent authenticity of its front explains why some reputable companies ended up being scammed by this group. Bottom line: be careful who you buy ads or install from: If it is too good to be true...</p> <p>In the <a src="https://www.elie.net/blog/security/taking-down-gooligan-a-retrospective-analysis-part-3-monetization-and-clean-up">final post</a> of the serie, I discusses Gooligan various monetization schemas and its takedown. See you there!</p> <p>Thank you for reading this post till the end! If you enjoyed it, don’t forget to share it on your favorite social network so that your friends and colleagues can enjoy it too and learn about Gooligan.</p> <p>To get notified when my next post is online, follow me on <a href="https://twitter.com/elie">Twitter</a> , <a href="https://www.facebook.com/elieblog">Facebook</a> , <a href="https://plus.google.com/+ElieBursztein">Google+</a> , or <a href="https://www.linkedin.com/in/bursztein/">LinkedIn</a> . You can also get the full posts directly in your inbox by subscribing to the mailing list or via <a href="http://feeds.feedburner.com/ebursztein">RSS</a> .</p> <p>A bientôt!</p>
]]>
</content>
<link type="text/html" rel="alternate" href="https://www.elie.net/blog/security/taking-down-gooligan-part-2-inner-working?utm_source=rss"/>
<author>
<name>Elie Bursztein</name>
<email>feed@elie.net</email>
</author>
</entry>
<entry>
<published>2018-03-10T09:00:00Z</published>
<updated>2017-03-17T09:00:00Z</updated>
<id>
https://www.elie.net/blog/security/taking-down-gooligan-part-1-overview?utm_source=rss
</id>
<title>Taking down Gooligan: part 1 — overview</title>
<content type="html">
<![CDATA[
<p> <img src="https://www.elie.net/static/images/banner/taking-down-gooligan-part-1-overview.jpg"/> </p> <p>This series of posts recounts how, in November 2016, we hunted for and took down Gooligan, the infamous Android OAuth stealing botnet. What makes Gooligan special is its weaponization of OAuth tokens, something that was never observed in mainstream crimeware before. At its peak, Gooligan had hijacked over 1M OAuth tokens in an attempt to perform fraudulent Play store installs and reviews.</p> <blockquote><p>Gooligan marks a turning point in Android malware evolution as the first large scale OAuth crimeware</p> </blockquote><p>While I rarely talk publicly about it, a key function of our research team is to assist product teams when they face major attacks. Gooligan’s very public nature and the extensive cross-industry collaboration around its takedown provided the perfect opportunity to shed some light on this aspect of our mission.</p> <p>Being part of the emergency response task force is a central aspect of our team, as it allows us to focus on helping our users when they need it the most and exposes us to tough challenges in real time, as they occur. Overcoming these challenges fuels our understanding of the security and abuse landscape. Quite a few of our most successful research projects started due to these escalations, including our work on <a src="https://www.elie.net/publication/dialing-back-abuse-on-phone-verified-accounts">fake phone verified accounts</a> , the <a src="https://www.elie.net/blog/security/understanding-the-prevalence-of-web-traffic-interception">study of HTTPS interception</a> , and the analysis of <a src="https://www.elie.net/blog/understanding-how-tls-downgrade-attacks-prevent-email-encryption">mail delivery security</a> .</p> <p> <img src="https://www.elie.net/static/images/images/taking-down-gooligan-a-retrospective-analysis/gooligan-post-topics.png" alt="subject covered in this post"/> </p> <p>Given the complexity of this subject, I broke it down into three posts to ensure that I can provide a a full debrief of what went down and cover all the major aspects of the Gooligan escalation. This first post recounts the Gooligan origin story and offers an overview of how Gooligan works. The <a src="https://www.elie.net/blog/security/taking-down-gooligan-part-2-inner-working">second post</a> provides an in-depth analysis of Gooligan’s inner workings and an analysis of its network infrastructure. The <a src="https://www.elie.net/blog/security/taking-down-gooligan-a-retrospective-analysis-part-3-monetization-and-clean-up">final post</a> discusses Gooligan various monetization schemas and its takedown.</p> <p>This series of posts is modeled after the talk I gave with <a href="https://www.linkedin.com/in/oren-koriat-8b20a8120/">Oren Koriat</a> from Check Point, at <a href="https://www.botconf.eu/botconf-2017/">Botconf</a> in December 2017, on the subject. Here is a re-recording of the talk:</p> <p> <div class="video"> <iframe width="560" height="315" src="https://www.youtube.com/embed/gt1loXpk-0A" frameborder="0" allowfullscreen></iframe> </div> </p> <p>You can get the slides <a src="https://www.elie.net/talk/hunting-down-gooligan-a-retrospective-analysis">here</a> but they are pretty bare.</p> <p>As OAuth token abuse is Gooligan’s key innovation, let’s start by quickly summarizing how OAuth tokens work, so it is clear why this is such a game changer.</p> <h2>What are Oauth tokens?</h2><p> <img src="https://www.elie.net/static/images/images/taking-down-gooligan-a-retrospective-analysis/oauth-google-access-example.png" alt="Oauth app list"/> </p> <p> <a href="https://en.wikipedia.org/wiki/OAuth">OAuth</a> tokens are the de facto standard for granting apps and devices restricted access to online accounts without sharing passwords and with a limited set of privileges. For example, you can use an OAuth token to only allow an app to read your Twitter timeline, while preventing it from changing your settings or posting on your behalf.</p> <p> <img src="https://www.elie.net/static/images/images/taking-down-gooligan-a-retrospective-analysis/oauth-flow-illustrated.png" alt="OAuth flow"/> </p> <p> <a href="https://developers.google.com/identity/protocols/OAuth2">Under the hood</a> , the service provides the app, on your behalf, with an OAuth token that is tied to the exact privileges you want to grant. In a way that is similar but not exactly the same, when you sign up with your Google account on an Android device, Google gives the device a token that allows it to access Google services on your behalf. This is the long term token that Gooligan stole in order to impersonate users on the Play Store. You can read more about Android long term tokens <a href="https://developer.android.com/training/id-auth/authenticate.html">here</a> .</p> <h2>Overview</h2><p> <img src="https://www.elie.net/static/images/images/taking-down-gooligan-a-retrospective-analysis/gooligan-overview.png" alt="Gooligan overview"/> </p> <p>Overall, Gooligan is made of six key components:</p> <ul> <li><strong>Repackaged app</strong>: This is the initial payload, which is usually a popular repackaged app that was weaponized. This APK embedded a secondary hidden/encrypted payload. </li> <li><strong>Registration server</strong>: Record device information when it join the botnet after being rooted. </li> <li><strong>Exploit server</strong>: The exploit server is the system that will deliver the exact exploit needed to root the device, based on the information provided by the secondary payload. Having the device information is essential, as <a href="https://kingroot.net/">Kingroot</a> only targeted unpatched older devices (4.x and below). The post-rooting process is also responsible for backdooring the phone recovery process to enable persistence.</li> <li><strong>Fraudulent app and ads C&amp;C</strong>: This infrastructure is responsible for collecting exfiltrated data and telling the malware which (non-Google related) ads to display and which Play store app to boost.</li> <li><strong>Play Store app module</strong>: This is an injected library that allows the malware to issue commands to the Play store through the Play store app. This complex process was set up in an attempt to avoid triggering Play store protection.</li> <li><strong>Ads fraud module</strong>: This is a module that would regularly display ads to the users as an overlay. The ads were benign and came from an ad company that we couldn’t identify.</li> </ul> <h2>Genesis</h2><p>Analyzing Gooligan’s code allowed us to trace it back to earlier malware families, as it built upon their codebase. While those families are clearly related code-wise, we can't ascertain whether the same actor is behind all of them, because a lot of the shared features were extensively discussed in Chinese blogs.</p> <p> <img src="https://www.elie.net/static/images/images/taking-down-gooligan-a-retrospective-analysis/gooligan-timeline.png" alt="Gooligan timeline"/> </p> <h3>SnapPea the precursor</h3><p>As visible in the timeline above, Gooligan’s genesis can be traced back to the <a href="https://blog.checkpoint.com/2015/07/10/adware-or-apt-snappea-downloader-an-android-malware-that-implements-12-different-exploits/">SnapPea adware</a> that emerged in March 2015 and was discovered by Check Point in July of the same year. SnapPea’s key innovation was the weaponization of the exploit kit <a href="https://kingroot.net/">Kingroot</a> , which was until then used by enthusiasts to root their phones and install custom ROMs.</p> <p> <img src="https://www.elie.net/static/images/images/taking-down-gooligan-a-retrospective-analysis/snappea-malware-discovery.jpg" alt="Blog post announcing SnapPea discovery"/> </p> <p><em>SnapPea</em> Kingroot straightforward weaponization led to a rather unusual infection vector: its authors resorted to backdooring the backup application SnapPea to be able to infect victims. After an Android device was physically connected to an infected PC, the malicious SnapPea application used Kingroot to root the device in order to install malware on the device. Gooligan is related to <em>SnapPea</em> because Gooligan also use Kingroot exploits to root devices but in an untethered way via a custom remote server.</p> <blockquote><p>Following SnapPea footsteps Gooligan weaponizes the Kingroot exploits to root old unpatched Android devices.</p> </blockquote><h3>Ghost Push the role model</h3><p> <img src="https://www.elie.net/static/images/images/taking-down-gooligan-a-retrospective-analysis/ghost-push-discovered-android-authority.jpg" alt="Blog post discussing Ghost Push discovery"/> </p> <p>A few months after SnapPea appeared, <a href="http://www.cmcm.com/blog/en/security/2015-09-18/799.html">Cheetah Mobile uncovered</a> <a href="https://en.wikipedia.org/wiki/Ghost_Push"><em>Ghost Push</em></a> , which quickly became one of the largest Android (off-market) botnets. What set <em>Ghost Push</em> apart technically from <em>SnapPea</em> was the addition of code that allowed it to persist during the device reset. This persistence was accomplished by patching, among other things, the recovery script located in the system partition after <em>Ghost Push</em> gained root access in the same way Snappea did. Gooligan reused the same persistent code.</p> <blockquote><p>Gooligan borrowed from Ghost Push the code used to ensure its persistence across device resets.</p> </blockquote><h2>Wrap-up</h2><p>As outline in this post Gooligan is a complex malware that built on previous malware generation and extend it to a brand new vector of attack: OAuth tokens theft.</p> <blockquote><p>Gooligan marks a turning point in Android malware evolution as the first large scale OAuth crimeware</p> </blockquote><p>Building up on this post, the <a src="https://www.elie.net/blog/security/taking-down-gooligan-part-2-inner-working">next one</a> of the serie will provide in-depth analysis of Gooligan’s inner workings and an analysis of its network infrastructure. The <a src="https://www.elie.net/blog/security/taking-down-gooligan-a-retrospective-analysis-part-3-monetization-and-clean-up">final post</a> will discusses Gooligan various monetization schemas and its takedown</p> <p>Thank you for reading this post till the end! If you enjoyed it, don’t forget to share it on your favorite social network so that your friends and colleagues can enjoy it too and learn about Gooligan.</p> <p>To get notified when my next post is online, follow me on <a href="https://twitter.com/elie">Twitter</a> , <a href="https://www.facebook.com/elieblog">Facebook</a> , <a href="https://plus.google.com/+ElieBursztein">Google+</a> , or <a href="https://www.linkedin.com/in/bursztein/">LinkedIn</a> . You can also get the full posts directly in your inbox by subscribing to the mailing list or via <a href="http://feeds.feedburner.com/ebursztein">RSS</a> .</p> <p>A bientôt!</p>
]]>
</content>
<link type="text/html" rel="alternate" href="https://www.elie.net/blog/security/taking-down-gooligan-part-1-overview?utm_source=rss"/>
<author>
<name>Elie Bursztein</name>
<email>feed@elie.net</email>
</author>
</entry>
<entry>
<published>2018-03-10T00:00:00Z</published>
<updated>2018-03-17T00:00:00Z</updated>
<id>
https://www.elie.net/talk/hunting-down-gooligan-a-retrospective-analysis?utm_source=rss
</id>
<title>Hunting down Gooligan — retrospective analysis</title>
<content type="html">
<![CDATA[
<p> <img src="https://www.elie.net/static/images/banner/hunting-down-gooligan-a-retrospective-analysis.jpg"/> </p> <p>This talk provides a retrospective on how during 2017 Check Point and Google jointly hunted down Gooligan – one of the largest Android botnets at the time. Beside its scale what makes Gooligan a worthwhile case-study is its heavy reliance on stolen oauth tokens to attack Google Play’s API, an approach previously unheard of in malware.</p> <p>This talk starts by providing an in-depth analysis of how Gooligan’s kill-chain works from infection and exploitation to system-wide compromise. Then building on various telemetry we will shed light on which devices were infected and how this botnet attempted to monetize the stolen oauth tokens. Next we will discuss how we were able to uncover the Gooligan infrastructure and how we were able to tie it to another prominent malware family: Ghostpush. Last but not least we will recount how we went about re-securing the affected users and takedown the infrastructure.</p>
]]>
</content>
<link type="text/html" rel="alternate" href="https://www.elie.net/talk/hunting-down-gooligan-a-retrospective-analysis?utm_source=rss"/>
<author>
<name>Elie Bursztein</name>
<email>feed@elie.net</email>
</author>
</entry>
<entry>
<published>2018-03-02T10:00:00Z</published>
<updated>2018-03-18T10:00:00Z</updated>
<id>
https://www.elie.net/publication/tracking-ransomware-end-to-end?utm_source=rss
</id>
<title>Tracking desktop ransomware payments end to end</title>
<content type="html">
<![CDATA[
<p> <img src="https://www.elie.net/static/images/banner/tracking-ransomware-end-to-end.jpg"/> </p> <p>Ransomware is a type of malware that encrypts the files of infected hosts and demands payment, often in a crypto-currency such as Bitcoin. In this paper, we create a measurement framework that we use to perform a large-scale, two-year, end-to-end measurement of ransomware payments, victims, and operators. By combining an array of data sources, including ransomware binaries, seed ransom payments, victim telemetry from infections, and a large database of Bitcoin addresses annotated with their owners, we sketch the outlines of this burgeoning ecosystem and associated third-party infrastructure.</p> <p>In particular, we trace the financial transactions, from the moment victims acquire bitcoins, to when ransomware operators cash them out. We find that many ransomware operators cashed out using BTC-e, a now-defunct Bitcoin exchange. In total we are able to track over $16 million in likely ransom payments made by 19,750 potential victims during a two-year period.</p> <p>While our study focuses on ransomware, our methods are potentially applicable to other cybercriminal operations that have similarly adopted Bitcoin as their payment channel.</p>
]]>
</content>
<link type="text/html" rel="alternate" href="https://www.elie.net/publication/tracking-ransomware-end-to-end?utm_source=rss"/>
<author>
<name>Elie Bursztein</name>
<email>feed@elie.net</email>
</author>
</entry>
</feed>
